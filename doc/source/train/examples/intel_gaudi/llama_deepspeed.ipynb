{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Llama-2 Model with Deepspeed on Intel Gaudi\n",
    "\n",
    "In this Jupyter notebook, we will fine-tune a [Llama-2-70b](https://huggingface.co/meta-llama/Llama-2-70b-chat-hf) model with Deepspeed on Intel Gaudi. We will use PyTorch for model training and Ray for distributed training. We will use dataset [tatsu-lab/alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca).\n",
    "\n",
    "[Intel Gaudi AI Processors (HPUs)](https://habana.ai) are AI hardware accelerators designed by Habana Labs. For more information, see [Gaudi Architecture](https://docs.habana.ai/en/latest/Gaudi_Overview/index.html) and [Gaudi Developer Docs](https://developer.habana.ai/).\n",
    "\n",
    "Basic features for this fine-tuning example are:\n",
    "- Running on HPUs, support three execution mode: [\"lazy\", \"eager\", \"eager.compile\"](https://docs.habana.ai/en/latest/PyTorch/Reference/PyTorch_Gaudi_Theory_of_Operations.html).\n",
    "- Deepspeed integrated and LoRA training.\n",
    "- [`GaudiTrainer`](https://github.com/huggingface/optimum-habana/blob/main/optimum/habana/transformers/trainer.py) based training.\n",
    "- Llama-2-70b model.\n",
    "- Ray based scheduling and management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare environment\n",
    "This example run on single node with 8 HPUs.\n",
    "\n",
    "We recommend using a prebuilt container to run these examples. To run a container, you need Docker. See [Install Docker Engine](https://docs.docker.com/engine/install/) for installation instructions.\n",
    "\n",
    "Next, follow [Run Using Containers](https://docs.habana.ai/en/latest/Installation_Guide/Bare_Metal_Fresh_OS.html?highlight=installer#run-using-containers) to install the Habana drivers and container runtime.\n",
    "\n",
    "### Get docker image\n",
    "``` bash\n",
    "docker pull vault.habana.ai/gaudi-docker/1.15.1/ubuntu22.04/habanalabs/pytorch-installer-2.2.0:latest\n",
    "```\n",
    "### Run docker image\n",
    "``` bash\n",
    "docker run -it --runtime=habana -e HABANA_VISIBLE_DEVICES=all -e OMPI_MCA_btl_vader_single_copy_mechanism=none --cap-add=sys_nice --net=host --ipc=host vault.habana.ai/gaudi-docker/1.15.1/ubuntu22.04/habanalabs/pytorch-installer-2.2.0:latest\n",
    "# maybe should mapping your workspace volumns\n",
    "```\n",
    "### Install dependency\n",
    "``` bash\n",
    "# for exection mode \"eager\" or \"eager.compile\", please install \"optimum-habana>1.11.1\"\n",
    "pip install ray[train] notebook transformers datasets evaluate peft accelerate scikit-learn optimum-habana\n",
    "# install deepspeed\n",
    "pip install git+https://github.com/HabanaAI/DeepSpeed.git@1.15.1\n",
    "\n",
    "# this notebook verfied with packages' version:\n",
    "# transformers==4.38.2\n",
    "# datasets==2.19.1\n",
    "# evaluate==0.4.2\n",
    "# peft==0.4.0\n",
    "# accelerate==0.27.2\n",
    "# scikit-learn==1.4.2\n",
    "# optimum-habana==1.11.1\n",
    "# deepspeed==0.12.4+hpu.synapse.v1.15.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:319: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:319: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import time\n",
    "from typing import Dict\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import datasets\n",
    "import transformers\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    AutoModelForSequenceClassification,\n",
    ")\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import peft\n",
    "\n",
    "from optimum.habana import GaudiTrainer, GaudiConfig, GaudiTrainingArguments\n",
    "from optimum.habana.transformers.modeling_utils import adapt_transformers_to_gaudi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset Function\n",
    "\n",
    "Preprocessing the raw dataset's each line with specified format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_dataset(raw_datasets):\n",
    "\n",
    "    PROMPT_DICT = {\n",
    "        \"prompt_with_input\": (\n",
    "            \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "            \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n",
    "        ),\n",
    "        \"prompt_without_input\": (\n",
    "            \"Below is an instruction that describes a task. \"\n",
    "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "            \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    def create_prompts(examples):\n",
    "        prompts = {}\n",
    "        prompts[\"source\"] = []\n",
    "        prompts[\"target\"] = []\n",
    "        for example in examples:\n",
    "            prompt_template = (\n",
    "                PROMPT_DICT[\"prompt_with_input\"] if example[\"input\"] != \"\" else PROMPT_DICT[\"prompt_without_input\"]\n",
    "            )\n",
    "            source = prompt_template.format_map(example)\n",
    "            prompts[\"source\"].append(source)\n",
    "            prompts[\"target\"].append(example[\"output\"])\n",
    "        return prompts\n",
    "\n",
    "    # Preprocessing the datasets.\n",
    "    for key in raw_datasets:\n",
    "        prompts = create_prompts(raw_datasets[key])\n",
    "        columns_to_be_removed = list(raw_datasets[key].features.keys())\n",
    "        raw_datasets[key] = raw_datasets[key].add_column(\"prompt_sources\", prompts[\"source\"])\n",
    "        raw_datasets[key] = raw_datasets[key].add_column(\"prompt_targets\", prompts[\"target\"])\n",
    "        raw_datasets[key] = raw_datasets[key].remove_columns(columns_to_be_removed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset to Tokenizer Function\n",
    "\n",
    "Tokenize each line in dataset by model tokenizer.\n",
    "\n",
    "In example codes, we concatenate the dataset's line content to accelerate training speed.\n",
    "\n",
    "All datasets are processed as \"train\" datasets, no evaluation datasets are sampled from raw_datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_dataset_to_tokenizer(raw_datasets, tokenizer):\n",
    "    max_seq_length = 512\n",
    "    tokenizer.pad_token_id = 0\n",
    "    tokenizer.eos_token_id = 1\n",
    "    tokenizer.bos_token_id = 2\n",
    "\n",
    "    def tokenize(prompt, add_eos_token=True):\n",
    "        results = tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=max_seq_length,\n",
    "            padding=False,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        for i in range(len(results[\"input_ids\"])):\n",
    "            if (\n",
    "                results[\"input_ids\"][i][-1] != tokenizer.eos_token_id\n",
    "                and len(results[\"input_ids\"][i]) < max_seq_length\n",
    "                and add_eos_token\n",
    "            ):\n",
    "                results[\"input_ids\"][i].append(tokenizer.eos_token_id)\n",
    "                results[\"attention_mask\"][i].append(1)\n",
    "\n",
    "        results[\"labels\"] = copy.deepcopy(results[\"input_ids\"])\n",
    "        results[\"input_id_len\"] = [len(result) for result in results[\"input_ids\"]]\n",
    "        return results\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        keys = list(examples.data.keys())\n",
    "        if len(keys) != 2:\n",
    "            raise ValueError(\"Unsupported dataset format\")\n",
    "\n",
    "        st = [s + t for s, t in zip(examples[keys[0]], examples[keys[1]])]\n",
    "\n",
    "        examples_tokenized = tokenize(st)\n",
    "        input_ids = examples_tokenized[\"input_ids\"]\n",
    "        labels = examples_tokenized[\"labels\"]\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"labels\": labels,\n",
    "            \"attention_mask\": examples_tokenized[\"attention_mask\"],\n",
    "        }\n",
    "\n",
    "    tokenized_datasets = raw_datasets.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        load_from_cache_file=True,\n",
    "    )\n",
    "\n",
    "    def concatenate_data(dataset, max_seq_length):\n",
    "        concatenated_dataset = {}\n",
    "        for column in dataset.features:\n",
    "            concatenated_data = [item for sample in dataset[column] for item in sample]\n",
    "            reshaped_data = [\n",
    "                concatenated_data[i * max_seq_length : (i + 1) * max_seq_length]\n",
    "                for i in range(len(concatenated_data) // max_seq_length)\n",
    "            ]\n",
    "            concatenated_dataset[column] = reshaped_data\n",
    "        return datasets.Dataset.from_dict(concatenated_dataset)\n",
    "\n",
    "    tokenized_datasets_ = tokenized_datasets[\"train\"].remove_columns([\"prompt_sources\", \"prompt_targets\"])\n",
    "    tokenized_datasets[\"train\"] = concatenate_data(tokenized_datasets_, max_seq_length)\n",
    "\n",
    "    return tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training arguments\n",
    "\n",
    "By instance object of `GaudiTrainingArguments`, the essential of initialization HPU will be called, such as HPU device spcification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_training_args(config: Dict):\n",
    "    # prepare execution mode config\n",
    "    execution_mode = config[\"execution_mode\"]\n",
    "    use_lazy_mode = True if execution_mode == \"lazy\" else False\n",
    "    torch_compile_backend = \"hpu_backend\" if execution_mode == \"eager.compile\" else None\n",
    "\n",
    "    return GaudiTrainingArguments(deepspeed=config[\"deepspeed\"],\n",
    "                                  output_dir=config[\"output\"],\n",
    "                                  do_train=True,\n",
    "                                  do_eval=False,\n",
    "                                  per_device_train_batch_size=config[\"batch_size_per_worker\"],\n",
    "                                  bf16=True,\n",
    "                                  learning_rate=config[\"lr\"],\n",
    "                                  save_strategy=\"no\",\n",
    "                                  torch_compile_backend=torch_compile_backend,\n",
    "                                  evaluation_strategy=\"no\",\n",
    "                                  lr_scheduler_type=\"cosine\",\n",
    "                                  num_train_epochs=config[\"epochs\"],\n",
    "                                  use_lazy_mode=use_lazy_mode,\n",
    "                                  use_habana=True,\n",
    "                                  pipelining_fwd_bwd=True,\n",
    "                                  save_only_model=True,\n",
    "                                  gradient_checkpointing=True,\n",
    "                                  warmup_ratio=0.03,\n",
    "                                  throughput_warmup_steps=3,\n",
    "                                  logging_steps=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function\n",
    "\n",
    "This function will be executed by each worker during training, with following steps:\n",
    "\n",
    "- loading datasets and preprocess datasets, just load the first 4096 item as training datasets.\n",
    "- loading pretrained model as tokenizer, and process datasets to tokenizer.\n",
    "- loading pretrained model, convert to lora model, and move model to HPU device.\n",
    "- preparing data collator.\n",
    "- preparing training args, an instance of `GaudiTrainingArguments`.\n",
    "- preparing instance of `GaudiTrainer`.\n",
    "- calling `train()` to train model.\n",
    "- saving model results.\n",
    "\n",
    "\n",
    "Compared to a training function for GPU, no changes are needed to port to HPU. Internally, Ray Train does these things:\n",
    "\n",
    "- Detect HPU and set the device.\n",
    "- Initialize the habana PyTorch backend.\n",
    "- Initialize the habana distributed backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_func_per_worker(config: Dict):\n",
    "    # adapt transformers to gaudi\n",
    "    adapt_transformers_to_gaudi()\n",
    "\n",
    "    # prepare training arguments\n",
    "    training_args = prepare_training_args(config)\n",
    "\n",
    "    # prepare datasets\n",
    "    # here we use dataset \"tatsu-lab/alpaca\" from huggingface\n",
    "    # and sample some part\n",
    "    raw_datasets = datasets.DatasetDict({\"train\": datasets.load_dataset(\"tatsu-lab/alpaca\", split='train[0:4096]')})\n",
    "    preprocess_dataset(raw_datasets)\n",
    "\n",
    "    # prepare tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config[\"model\"])\n",
    "    tokenized_datasets = preprocess_dataset_to_tokenizer(raw_datasets, tokenizer)\n",
    "\n",
    "    # prepare model\n",
    "    if config[\"deepspeed\"] is not None:\n",
    "        auto_config = AutoConfig.from_pretrained(config[\"model\"], use_cache=False, revision=\"main\", use_auth_token=None, trust_remote_code=None)\n",
    "        model = AutoModelForCausalLM.from_pretrained(config[\"model\"], config=auto_config, **config[\"model_config\"])\n",
    "        model.generation_config.attn_softmax_bf16 = True\n",
    "        model.generation_config.use_flash_attention = True\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(config[\"model\"], **config[\"model_config\"])\n",
    "\n",
    "    peft_config = peft.LoraConfig(**config[\"lora_config\"])\n",
    "    model.enable_input_require_grads()\n",
    "    model = peft.get_peft_model(model, peft_config)\n",
    "    device = training_args.device\n",
    "    model.to(dtype=config[\"model_config\"][\"torch_dtype\"], device=device)\n",
    "\n",
    "    # prepare data collator\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", mlm=False)\n",
    "\n",
    "    gaudi_config = GaudiConfig()\n",
    "    gaudi_config.use_fused_adam = True\n",
    "    gaudi_config.use_fused_clip_norm = True\n",
    "\n",
    "    trainer = GaudiTrainer(\n",
    "        model=model,\n",
    "        gaudi_config=gaudi_config,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=None,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=None,\n",
    "        preprocess_logits_for_metrics=None,\n",
    "    )\n",
    "\n",
    "    train_result = trainer.train()\n",
    "    print(f\"train_result = {train_result}\")\n",
    "    trainer.save_model()\n",
    "\n",
    "    return train_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Training Function\n",
    "The `train_llama` function sets up the distributed training environment using Ray and starts the training process. To enable training using HPU, we only need to make the following changes:\n",
    "- Set the exectuion mode for training, supported execution mode are:\n",
    "\n",
    "    - \"lazy\": Deferred execution of graphs, comprising of ops delivered from script op by op similar to Eager mode. It gives the Eager mode experience with performance on Gaudi. Unlike Eager Mode with torch.compile, graph is analyzed in each iteration leading to a higher CPU usage.\n",
    "    - \"eager\": Op-by-op execution as defined in standard PyTorch Eager mode scripts.\n",
    "    - \"eager.compile\": Eager mode extended with `torch.compile` - Similar to Eager mode but extended with wrapping complete or part of model (such as a function) into a graph. Parts that are not wrapped are executed eagerly.\n",
    "\n",
    "    More detail theory can be found [here](https://docs.habana.ai/en/latest/PyTorch/Reference/PyTorch_Gaudi_Theory_of_Operations.html), and detail performance results can be found [here](https://developer.habana.ai/get-started/habana-models-performance/)\n",
    "- Require an HPU for each worker in ScalingConfig\n",
    "- Set backend to `hccl` in TorchConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_llama(num_workers, execution_mode):\n",
    "    import ray\n",
    "    from ray.train import ScalingConfig\n",
    "    from ray.train.torch import TorchTrainer, TorchConfig\n",
    "\n",
    "    # deepspeed config, can also place it to config file\n",
    "    deepspeed_config = {\n",
    "        \"steps_per_print\": 64,\n",
    "        \"train_batch_size\": \"auto\",\n",
    "        \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "        \"gradient_accumulation_steps\": \"auto\",\n",
    "        \"bf16\": {\n",
    "            \"enabled\": True\n",
    "        },\n",
    "        \"gradient_clipping\": 1.0,\n",
    "        \"zero_optimization\": {\n",
    "            \"stage\": 3,\n",
    "            \"overlap_comm\": False,\n",
    "            \"contiguous_gradients\": False,\n",
    "            \"stage3_gather_16bit_weights_on_model_save\": True\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Preparing train configurations\n",
    "    train_config = {\n",
    "        \"execution_mode\": execution_mode,\n",
    "        \"model\": \"/root/models/models--meta-llama--Llama-2-70b-chat-hf/snapshots/e9149a12809580e8602995856f8098ce973d1080/\",\n",
    "        \"model_config\": {\"torch_dtype\": torch.bfloat16, \"trust_remote_code\": None, \"use_auth_token\": None},\n",
    "        \"lora_config\": {\"task_type\": \"CAUSAL_LM\", \"r\": 4, \"lora_dropout\": 0.1, \"target_modules\": [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]},\n",
    "        \"lr\": 0.0018,\n",
    "        \"epochs\": 2,\n",
    "        \"batch_size_per_worker\": 10,\n",
    "        \"output\": \"/tmp/ray/\",\n",
    "        \"deepspeed\": deepspeed_config,\n",
    "    }\n",
    "\n",
    "    # Configure computation resources\n",
    "    # In ScalingConfig, require an HPU for each worker\n",
    "    scaling_config = ScalingConfig(num_workers=num_workers,\n",
    "                                   use_gpu=False,\n",
    "                                   resources_per_worker={\"CPU\": 1, \"HPU\": 1})\n",
    "    # Set backend to hccl in TorchConfig\n",
    "    torch_config = TorchConfig(backend=\"hccl\")\n",
    "\n",
    "    # start your ray cluster\n",
    "    ray.init()\n",
    "\n",
    "    # Initialize a Ray TorchTrainer\n",
    "    trainer = TorchTrainer(\n",
    "        train_loop_per_worker=train_func_per_worker,\n",
    "        train_loop_config=train_config,\n",
    "        torch_config=torch_config,\n",
    "        scaling_config=scaling_config,\n",
    "    )\n",
    "\n",
    "    result = trainer.fit()\n",
    "    print(f\"Training result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training\n",
    "\n",
    "Finally, we call the `train_llama` function to start the training process. You can adjust the number of workers to use, and the execution mode for HPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-08 01:35:23,594\tINFO worker.py:1749 -- Started a local Ray instance.\n",
      "2024-05-08 01:35:24,779\tINFO tune.py:614 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-05-08 01:35:24 (running for 00:00:00.11)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 0.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=699623)\u001b[0m /usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "\u001b[36m(pid=699623)\u001b[0m   _torch_pytree._register_pytree_node(\n",
      "\u001b[36m(TrainTrainable pid=699623)\u001b[0m /usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:319: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "\u001b[36m(TrainTrainable pid=699623)\u001b[0m   _torch_pytree._register_pytree_node(\n",
      "\u001b[36m(TrainTrainable pid=699623)\u001b[0m /usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:319: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "\u001b[36m(TrainTrainable pid=699623)\u001b[0m   _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-05-08 01:35:29 (running for 00:00:05.15)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=700358)\u001b[0m /usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "\u001b[36m(RayTrainWorker pid=700358)\u001b[0m   _torch_pytree._register_pytree_node(\n",
      "\u001b[36m(RayTrainWorker pid=700363)\u001b[0m /usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "\u001b[36m(RayTrainWorker pid=700363)\u001b[0m   _torch_pytree._register_pytree_node(\n",
      "\u001b[36m(RayTrainWorker pid=700357)\u001b[0m Setting up process group for: env:// [rank=0, world_size=8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-05-08 01:35:34 (running for 00:00:10.16)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TorchTrainer pid=699623)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=699623)\u001b[0m - (ip=100.83.111.228, pid=700357) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=699623)\u001b[0m - (ip=100.83.111.228, pid=700358) world_rank=1, local_rank=1, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=699623)\u001b[0m - (ip=100.83.111.228, pid=700359) world_rank=2, local_rank=2, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=699623)\u001b[0m - (ip=100.83.111.228, pid=700360) world_rank=3, local_rank=3, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=699623)\u001b[0m - (ip=100.83.111.228, pid=700361) world_rank=4, local_rank=4, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=699623)\u001b[0m - (ip=100.83.111.228, pid=700362) world_rank=5, local_rank=5, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=699623)\u001b[0m - (ip=100.83.111.228, pid=700363) world_rank=6, local_rank=6, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=699623)\u001b[0m - (ip=100.83.111.228, pid=700364) world_rank=7, local_rank=7, node_rank=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-05-08 01:35:39 (running for 00:00:15.18)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=700364)\u001b[0m /usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/hpu/__init__.py:158: UserWarning: torch.hpu.setDeterministic is deprecated and will be removed in next release. Please use torch.use_deterministic_algorithms instead.\n",
      "\u001b[36m(RayTrainWorker pid=700364)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(RayTrainWorker pid=700362)\u001b[0m /usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:319: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\u001b[32m [repeated 22x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=700362)\u001b[0m   _torch_pytree._register_pytree_node(\u001b[32m [repeated 22x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=700363)\u001b[0m [2024-05-08 01:35:42,394] [INFO] [real_accelerator.py:178:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "\u001b[36m(RayTrainWorker pid=700363)\u001b[0m [2024-05-08 01:35:42,573] [WARNING] [comm.py:163:init_deepspeed_backend] HCCL backend in DeepSpeed not yet implemented\n",
      "\u001b[36m(RayTrainWorker pid=700363)\u001b[0m [2024-05-08 01:35:42,573] [INFO] [comm.py:637:init_distributed] cdb=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/4096 [00:00<?, ? examples/s]\n",
      "Map:  24%|██▍       | 1000/4096 [00:00<00:00, 7068.02 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-05-08 01:35:44 (running for 00:00:20.20)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  98%|█████████▊| 4000/4096 [00:00<00:00, 5056.81 examples/s]\n",
      "Map: 100%|██████████| 4096/4096 [00:00<00:00, 5347.28 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-05-08 01:35:50 (running for 00:00:25.22)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=700358)\u001b[0m /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/partition_parameters.py:238: UserWarning: \"hpu:X\" notation is not supported by Gaudi PyTorch intergration bridge. Please change to \"hpu\" without index (Triggered internally at /npu-stack/pytorch-integration/pytorch_helpers/lazy_to_backend.cpp:53.)\n",
      "\u001b[36m(RayTrainWorker pid=700358)\u001b[0m   tensor: Tensor = fn(*args, **kwargs)\n",
      "\u001b[36m(RayTrainWorker pid=700362)\u001b[0m /usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/hpu/__init__.py:158: UserWarning: torch.hpu.setDeterministic is deprecated and will be removed in next release. Please use torch.use_deterministic_algorithms instead.\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=700362)\u001b[0m   warnings.warn(\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Map:   0%|          | 0/4096 [00:00<?, ? examples/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Map:  73%|███████▎  | 3000/4096 [00:00<00:00, 7674.50 examples/s]\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "Map: 100%|██████████| 4096/4096 [00:00<00:00, 5443.58 examples/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Map: 100%|██████████| 4096/4096 [00:00<00:00, 5321.53 examples/s]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=700357)\u001b[0m ============================= HABANA PT BRIDGE CONFIGURATION =========================== \n",
      "\u001b[36m(RayTrainWorker pid=700357)\u001b[0m  PT_HPU_LAZY_MODE = 1\n",
      "\u001b[36m(RayTrainWorker pid=700357)\u001b[0m  PT_RECIPE_CACHE_PATH = \n",
      "\u001b[36m(RayTrainWorker pid=700357)\u001b[0m  PT_CACHE_FOLDER_DELETE = 0\n",
      "\u001b[36m(RayTrainWorker pid=700357)\u001b[0m  PT_HPU_RECIPE_CACHE_CONFIG = \n",
      "\u001b[36m(RayTrainWorker pid=700357)\u001b[0m  PT_HPU_MAX_COMPOUND_OP_SIZE = 10\n",
      "\u001b[36m(RayTrainWorker pid=700357)\u001b[0m  PT_HPU_LAZY_ACC_PAR_MODE = 1\n",
      "\u001b[36m(RayTrainWorker pid=700357)\u001b[0m  PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      "\u001b[36m(RayTrainWorker pid=700357)\u001b[0m ---------------------------: System Configuration :---------------------------\n",
      "\u001b[36m(RayTrainWorker pid=700357)\u001b[0m Num CPU Cores : 152\n",
      "\u001b[36m(RayTrainWorker pid=700357)\u001b[0m CPU RAM       : 1056440348 KB\n",
      "\u001b[36m(RayTrainWorker pid=700357)\u001b[0m ------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=700357)\u001b[0m [2024-05-08 01:35:53,804] [INFO] [partition_parameters.py:345:__exit__] finished initializing model - num_params = 723, num_elems = 68.98B\n",
      "\u001b[36m(RayTrainWorker pid=700358)\u001b[0m [2024-05-08 01:35:42,394] [INFO] [real_accelerator.py:178:get_accelerator] Setting ds_accelerator to hpu (auto detect)\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=700358)\u001b[0m [2024-05-08 01:35:42,572] [WARNING] [comm.py:163:init_deepspeed_backend] HCCL backend in DeepSpeed not yet implemented\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=700358)\u001b[0m [2024-05-08 01:35:42,572] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[32m [repeated 7x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-05-08 01:35:55 (running for 00:00:30.24)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:36:00 (running for 00:00:35.25)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:36:05 (running for 00:00:40.27)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   7%|▋         | 1/15 [00:11<02:44, 11.73s/it]\n",
      "\u001b[36m(RayTrainWorker pid=700364)\u001b[0m /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/partition_parameters.py:238: UserWarning: \"hpu:X\" notation is not supported by Gaudi PyTorch intergration bridge. Please change to \"hpu\" without index (Triggered internally at /npu-stack/pytorch-integration/pytorch_helpers/lazy_to_backend.cpp:53.)\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=700364)\u001b[0m   tensor: Tensor = fn(*args, **kwargs)\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[32m [repeated 7x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-05-08 01:36:10 (running for 00:00:45.29)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:36:15 (running for 00:00:50.31)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:  13%|█▎        | 2/15 [00:23<02:29, 11.52s/it]\u001b[32m [repeated 8x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-05-08 01:36:20 (running for 00:00:55.32)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:36:25 (running for 00:01:00.34)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:  20%|██        | 3/15 [00:34<02:18, 11.53s/it]\u001b[32m [repeated 8x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-05-08 01:36:30 (running for 00:01:05.36)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:36:35 (running for 00:01:10.38)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:  27%|██▋       | 4/15 [00:45<02:05, 11.42s/it]\u001b[32m [repeated 8x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-05-08 01:36:40 (running for 00:01:15.39)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:36:45 (running for 00:01:20.41)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:36:50 (running for 00:01:25.43)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:  33%|███▎      | 5/15 [00:57<01:53, 11.37s/it]\u001b[32m [repeated 8x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-05-08 01:36:55 (running for 00:01:30.44)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:37:00 (running for 00:01:35.46)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:  40%|████      | 6/15 [01:08<01:42, 11.39s/it]\u001b[32m [repeated 8x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-05-08 01:37:05 (running for 00:01:40.48)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:37:10 (running for 00:01:45.49)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:  47%|████▋     | 7/15 [01:20<01:31, 11.45s/it]\u001b[32m [repeated 8x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-05-08 01:37:15 (running for 00:01:50.51)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:37:20 (running for 00:01:55.53)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:37:25 (running for 00:02:00.55)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:  53%|█████▎    | 8/15 [01:31<01:20, 11.44s/it]\u001b[32m [repeated 8x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-05-08 01:37:30 (running for 00:02:05.56)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:37:35 (running for 00:02:10.58)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:  60%|██████    | 9/15 [01:43<01:08, 11.44s/it]\u001b[32m [repeated 8x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-05-08 01:37:40 (running for 00:02:15.60)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:37:45 (running for 00:02:20.62)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:  67%|██████▋   | 10/15 [01:54<00:57, 11.45s/it]\u001b[32m [repeated 8x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-05-08 01:37:50 (running for 00:02:25.63)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:37:55 (running for 00:02:30.65)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:  73%|███████▎  | 11/15 [02:06<00:45, 11.49s/it]\u001b[32m [repeated 8x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-05-08 01:38:00 (running for 00:02:35.67)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:38:05 (running for 00:02:40.68)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:38:10 (running for 00:02:45.70)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:  80%|████████  | 12/15 [02:17<00:34, 11.50s/it]\u001b[32m [repeated 8x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-05-08 01:38:15 (running for 00:02:50.71)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:38:20 (running for 00:02:55.73)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:  87%|████████▋ | 13/15 [02:28<00:22, 11.46s/it]\u001b[32m [repeated 8x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-05-08 01:38:25 (running for 00:03:00.74)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:38:30 (running for 00:03:05.76)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:  93%|█████████▎| 14/15 [02:40<00:11, 11.33s/it]\n",
      "Loading checkpoint shards:  87%|████████▋ | 13/15 [02:29<00:22, 11.46s/it]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Loading checkpoint shards: 100%|██████████| 15/15 [02:40<00:00, 10.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-05-08 01:38:35 (running for 00:03:10.78)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=700357)\u001b[0m Parameter Offload: Total persistent parameters: 17702912 in 801 params\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:38:40 (running for 00:03:15.79)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/26 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:  93%|█████████▎| 14/15 [02:40<00:11, 11.33s/it]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Loading checkpoint shards: 100%|██████████| 15/15 [02:40<00:00, 10.73s/it]\u001b[32m [repeated 7x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-05-08 01:38:45 (running for 00:03:20.81)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:38:50 (running for 00:03:25.83)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:38:55 (running for 00:03:30.85)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:39:00 (running for 00:03:35.86)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:39:05 (running for 00:03:40.89)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:39:10 (running for 00:03:45.90)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:39:15 (running for 00:03:50.92)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:39:20 (running for 00:03:55.94)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:39:25 (running for 00:04:00.97)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:39:30 (running for 00:04:05.99)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:39:35 (running for 00:04:11.01)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:39:40 (running for 00:04:16.04)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:39:45 (running for 00:04:21.06)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:39:50 (running for 00:04:26.07)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:39:55 (running for 00:04:31.09)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:40:00 (running for 00:04:36.11)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:40:05 (running for 00:04:41.13)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:40:10 (running for 00:04:46.14)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:40:15 (running for 00:04:51.16)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:40:20 (running for 00:04:56.18)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:40:25 (running for 00:05:01.20)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:40:31 (running for 00:05:06.21)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:40:36 (running for 00:05:11.23)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:40:41 (running for 00:05:16.25)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:40:46 (running for 00:05:21.28)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/26 [02:07<53:15, 127.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-05-08 01:40:51 (running for 00:05:26.29)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:40:56 (running for 00:05:31.31)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:41:01 (running for 00:05:36.33)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:41:06 (running for 00:05:41.35)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:41:11 (running for 00:05:46.36)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:41:16 (running for 00:05:51.38)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:41:21 (running for 00:05:56.39)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:41:26 (running for 00:06:01.41)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:41:31 (running for 00:06:06.43)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:41:36 (running for 00:06:11.45)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:41:41 (running for 00:06:16.46)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:41:46 (running for 00:06:21.48)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:41:51 (running for 00:06:26.50)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:41:56 (running for 00:06:31.52)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:42:01 (running for 00:06:36.54)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:42:06 (running for 00:06:41.56)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:42:11 (running for 00:06:46.58)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:42:16 (running for 00:06:51.60)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:42:21 (running for 00:06:56.62)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:42:26 (running for 00:07:01.63)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:42:31 (running for 00:07:06.65)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:42:36 (running for 00:07:11.67)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2/26 [03:57<46:46, 116.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-05-08 01:42:41 (running for 00:07:16.69)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:42:46 (running for 00:07:21.71)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 3/26 [04:07<26:05, 68.06s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-05-08 01:42:51 (running for 00:07:26.72)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:42:56 (running for 00:07:31.74)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 4/26 [04:16<16:24, 44.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-05-08 01:43:01 (running for 00:07:36.75)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:43:06 (running for 00:07:41.77)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 5/26 [04:25<11:11, 31.96s/it]\n",
      " 19%|█▉        | 5/26 [04:25<11:11, 31.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=700357)\u001b[0m {'loss': 1.5577, 'grad_norm': 0.8229730129241943, 'learning_rate': 0.0016886760120394771, 'epoch': 0.38, 'memory_allocated (GB)': 16.85, 'max_memory_allocated (GB)': 29.34, 'total_memory_available (GB)': 94.62}\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:43:11 (running for 00:07:46.79)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:43:16 (running for 00:07:51.80)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 6/26 [04:34<08:03, 24.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-05-08 01:43:21 (running for 00:07:56.82)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 7/26 [04:43<06:06, 19.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-05-08 01:43:26 (running for 00:08:01.84)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:43:31 (running for 00:08:06.85)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 8/26 [04:52<04:48, 16.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-05-08 01:43:36 (running for 00:08:11.87)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:43:41 (running for 00:08:16.88)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 9/26 [05:01<03:54, 13.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-05-08 01:43:46 (running for 00:08:21.90)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:43:51 (running for 00:08:26.91)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 10/26 [05:10<03:16, 12.29s/it]\n",
      " 38%|███▊      | 10/26 [05:10<03:16, 12.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=700357)\u001b[0m {'loss': 1.1295, 'grad_norm': 0.18815693259239197, 'learning_rate': 0.0012832013624085653, 'epoch': 0.77, 'memory_allocated (GB)': 16.85, 'max_memory_allocated (GB)': 29.39, 'total_memory_available (GB)': 94.62}\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:43:56 (running for 00:08:31.93)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:44:01 (running for 00:08:36.94)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 11/26 [05:19<02:49, 11.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-05-08 01:44:06 (running for 00:08:41.96)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 12/26 [05:28<02:28, 10.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-05-08 01:44:11 (running for 00:08:46.98)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:44:16 (running for 00:08:52.00)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 13/26 [05:37<02:12, 10.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-05-08 01:44:21 (running for 00:08:57.02)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:44:26 (running for 00:09:02.03)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 14/26 [05:46<01:57,  9.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-05-08 01:44:31 (running for 00:09:07.05)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:44:36 (running for 00:09:12.06)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 15/26 [05:55<01:45,  9.55s/it]\n",
      " 58%|█████▊    | 15/26 [05:55<01:45,  9.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=700357)\u001b[0m {'loss': 0.9853, 'grad_norm': 0.1367674320936203, 'learning_rate': 0.0007313568168728476, 'epoch': 1.15, 'memory_allocated (GB)': 16.85, 'max_memory_allocated (GB)': 29.6, 'total_memory_available (GB)': 94.62}\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:44:41 (running for 00:09:17.08)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:44:46 (running for 00:09:22.10)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 16/26 [06:04<01:34,  9.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-05-08 01:44:51 (running for 00:09:27.11)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 17/26 [06:13<01:24,  9.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-05-08 01:44:56 (running for 00:09:32.13)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:45:01 (running for 00:09:37.14)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 18/26 [06:23<01:14,  9.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-05-08 01:45:06 (running for 00:09:42.16)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:45:11 (running for 00:09:47.17)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 19/26 [06:32<01:04,  9.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-05-08 01:45:16 (running for 00:09:52.19)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:45:21 (running for 00:09:57.21)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 20/26 [06:41<00:55,  9.24s/it]\n",
      " 77%|███████▋  | 20/26 [06:41<00:55,  9.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=700357)\u001b[0m {'loss': 0.9102, 'grad_norm': 0.07150674611330032, 'learning_rate': 0.0002439282353207298, 'epoch': 1.54, 'memory_allocated (GB)': 16.85, 'max_memory_allocated (GB)': 29.6, 'total_memory_available (GB)': 94.62}\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:45:27 (running for 00:10:02.22)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:45:32 (running for 00:10:07.24)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 21/26 [06:50<00:45,  9.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-05-08 01:45:37 (running for 00:10:12.26)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:45:42 (running for 00:10:17.27)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 22/26 [06:59<00:36,  9.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-05-08 01:45:47 (running for 00:10:22.29)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 23/26 [07:08<00:27,  9.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-05-08 01:45:52 (running for 00:10:27.30)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:45:57 (running for 00:10:32.33)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 24/26 [07:17<00:18,  9.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-05-08 01:46:02 (running for 00:10:37.34)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:46:07 (running for 00:10:42.36)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 25/26 [07:26<00:09,  9.08s/it]\n",
      " 96%|█████████▌| 25/26 [07:26<00:09,  9.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=700357)\u001b[0m {'loss': 0.8973, 'grad_norm': 0.07026992738246918, 'learning_rate': 7.096768816970011e-06, 'epoch': 1.92, 'memory_allocated (GB)': 16.85, 'max_memory_allocated (GB)': 29.6, 'total_memory_available (GB)': 94.62}\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:46:12 (running for 00:10:47.38)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:46:17 (running for 00:10:52.40)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [07:35<00:00,  9.04s/it]\n",
      "100%|██████████| 26/26 [07:35<00:00, 17.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=700363)\u001b[0m train_result = TrainOutput(global_step=26, training_loss=1.0912420657964854, metrics={'train_runtime': 455.8151, 'train_samples_per_second': 8.352, 'train_steps_per_second': 0.11, 'total_flos': 113117358981120.0, 'train_loss': 1.0912420657964854, 'epoch': 2.0, 'memory_allocated (GB)': 16.85, 'max_memory_allocated (GB)': 29.51, 'total_memory_available (GB)': 94.62})\n",
      "\u001b[36m(RayTrainWorker pid=700357)\u001b[0m {'train_runtime': 455.6666, 'train_samples_per_second': 8.352, 'train_steps_per_second': 0.11, 'train_loss': 1.0917885830769172, 'epoch': 2.0, 'memory_allocated (GB)': 16.85, 'max_memory_allocated (GB)': 29.6, 'total_memory_available (GB)': 94.62}\n",
      "\u001b[36m(RayTrainWorker pid=700357)\u001b[0m train_result = TrainOutput(global_step=26, training_loss=1.0917885830769172, metrics={'train_runtime': 455.6666, 'train_samples_per_second': 8.352, 'train_steps_per_second': 0.11, 'train_loss': 1.0917885830769172, 'epoch': 2.0, 'memory_allocated (GB)': 16.85, 'max_memory_allocated (GB)': 29.6, 'total_memory_available (GB)': 94.62})\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:46:22 (running for 00:10:57.41)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:46:27 (running for 00:11:02.43)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:46:32 (running for 00:11:07.45)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:46:37 (running for 00:11:12.47)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:46:42 (running for 00:11:17.48)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:46:47 (running for 00:11:22.50)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:46:52 (running for 00:11:27.51)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:46:57 (running for 00:11:32.53)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:47:02 (running for 00:11:37.54)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:47:07 (running for 00:11:42.56)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:47:12 (running for 00:11:47.57)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:47:17 (running for 00:11:52.59)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:47:22 (running for 00:11:57.61)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:47:27 (running for 00:12:02.62)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:47:32 (running for 00:12:07.64)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (0.0/1.0 TPU, 8.0/8.0 HPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:47:37 (running for 00:12:12.66)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=700357)\u001b[0m /usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /root/models/models--meta-llama--Llama-2-70b-chat-hf/snapshots/e9149a12809580e8602995856f8098ce973d1080/ - will assume that the vocabulary was not modified.\n",
      "\u001b[36m(RayTrainWorker pid=700357)\u001b[0m   warnings.warn(\n",
      "2024-05-08 01:47:39,967\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n",
      "2024-05-08 01:47:39,968\tINFO tune.py:1007 -- Wrote the latest version of all result files and experiment state to '/root/ray_results/TorchTrainer_2024-05-08_01-35-24' in 0.0025s.\n",
      "2024-05-08 01:47:39,970\tINFO tune.py:1039 -- Total run time: 735.19 seconds (735.17 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial TorchTrainer_3de91_00000 completed. Last result: \n",
      "== Status ==\n",
      "Current time: 2024-05-08 01:47:39 (running for 00:12:15.18)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 9.0/152 CPUs, 0/0 GPUs (8.0/8.0 HPU, 0.0/1.0 TPU)\n",
      "Result logdir: /tmp/ray/session_2024-05-08_01-35-20_406541_689603/artifacts/2024-05-08_01-35-24/TorchTrainer_2024-05-08_01-35-24/driver_artifacts\n",
      "Number of trials: 1/1 (1 TERMINATED)\n",
      "\n",
      "\n",
      "Training result: Result(\n",
      "  metrics={},\n",
      "  path='/root/ray_results/TorchTrainer_2024-05-08_01-35-24/TorchTrainer_3de91_00000_0_2024-05-08_01-35-24',\n",
      "  filesystem='local',\n",
      "  checkpoint=None\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# set some environment variables\n",
    "os.environ[\"RAY_EXPERIMENTAL_NOSET_HABANA_VISIBLE_MODULES\"] = \"0\"\n",
    "os.environ[\"PT_HPU_MAX_COMPOUND_OP_SIZE\"] = \"10\"\n",
    "os.environ[\"DEEPSPEED_HPU_ZERO3_SYNC_MARK_STEP_REQUIRED\"] = \"1\"\n",
    "# execution_mode are [\"lazy\", \"eager\", \"eager.compile\"]\n",
    "execution_mode = \"lazy\"\n",
    "os.environ[\"PT_HPU_LAZY_MODE\"] = \"1\" if execution_mode == \"lazy\" else \"0\"\n",
    "train_llama(num_workers=8, execution_mode=\"lazy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
