{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama-2 Model Training with HPU\n",
    "\n",
    "In this Jupyter notebook, we will fine-tune a [Llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) model by using HPU in DDP accelerate mode. We will use PyTorch for model training and Ray for distributed training. We will use dataset [tatsu-lab/alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca).\n",
    "\n",
    "[Habana Gaudi AI Processors (HPUs)](https://habana.ai) are AI hardware accelerators designed by Habana Labs. For more information, see [Gaudi Architecture](https://docs.habana.ai/en/latest/Gaudi_Overview/index.html) and [Gaudi Developer Docs](https://developer.habana.ai/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare environment\n",
    "A node with Gaudi/Gaudi2 installed is required to run this example. Both Gaudi and Gaudi2 have 8 HPUs. We will use 2 workers to train the model, each using 1 HPU.\n",
    "\n",
    "We recommend using a prebuilt container to run these examples. To run a container, you need Docker. See [Install Docker Engine](https://docs.docker.com/engine/install/) for installation instructions.\n",
    "\n",
    "Next, follow [Run Using Containers](https://docs.habana.ai/en/latest/Installation_Guide/Bare_Metal_Fresh_OS.html?highlight=installer#run-using-containers) to install the Habana drivers and container runtime.\n",
    "\n",
    "### Get docker image\n",
    "``` bash\n",
    "docker pull vault.habana.ai/gaudi-docker/1.15.1/ubuntu22.04/habanalabs/pytorch-installer-2.2.0:latest\n",
    "```\n",
    "### Run docker image\n",
    "``` bash\n",
    "docker run -it --runtime=habana -e HABANA_VISIBLE_DEVICES=all -v /home/wgs/projects:/root/workspace -v /opt/models/:/root/models -e OMPI_MCA_btl_vader_single_copy_mechanism=none --cap-add=sys_nice --net=host --ipc=host vault.habana.ai/gaudi-docker/1.15.1/ubuntu22.04/habanalabs/pytorch-installer-2.2.0:latest\n",
    "```\n",
    "### Install dependency\n",
    "``` bash\n",
    "pip install ray[train] notebook transformers datasets evaluate peft accelerate optimum-habana\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import time\n",
    "from typing import Dict\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    AutoModelForSequenceClassification,\n",
    ")\n",
    "\n",
    "import peft\n",
    "\n",
    "import ray.train\n",
    "from ray.train import ScalingConfig\n",
    "from ray.train.torch import TorchTrainer\n",
    "from ray.train.torch import TorchConfig\n",
    "from ray.runtime_env import RuntimeEnv\n",
    "\n",
    "import habana_frameworks.torch.core as htcore\n",
    "\n",
    "from optimum.habana.accelerate import GaudiAccelerator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_dataset(raw_datasets):\n",
    "\n",
    "    PROMPT_DICT = {\n",
    "        \"prompt_with_input\": (\n",
    "            \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "            \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n",
    "        ),\n",
    "        \"prompt_without_input\": (\n",
    "            \"Below is an instruction that describes a task. \"\n",
    "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "            \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    def create_prompts(examples):\n",
    "        prompts = {}\n",
    "        prompts[\"source\"] = []\n",
    "        prompts[\"target\"] = []\n",
    "        for example in examples:\n",
    "            prompt_template = (\n",
    "                PROMPT_DICT[\"prompt_with_input\"] if example[\"input\"] != \"\" else PROMPT_DICT[\"prompt_without_input\"]\n",
    "            )\n",
    "            source = prompt_template.format_map(example)\n",
    "            prompts[\"source\"].append(source)\n",
    "            prompts[\"target\"].append(example[\"output\"])\n",
    "        return prompts\n",
    "\n",
    "    # Preprocessing the datasets.\n",
    "    for key in raw_datasets:\n",
    "        prompts = create_prompts(raw_datasets[key])\n",
    "        columns_to_be_removed = list(raw_datasets[key].features.keys())\n",
    "        raw_datasets[key] = raw_datasets[key].add_column(\"prompt_sources\", prompts[\"source\"])\n",
    "        raw_datasets[key] = raw_datasets[key].add_column(\"prompt_targets\", prompts[\"target\"])\n",
    "        raw_datasets[key] = raw_datasets[key].remove_columns(columns_to_be_removed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset to Tokenizer Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_dataset_to_tokenizer(raw_datasets, tokenizer):\n",
    "    max_seq_length = 512\n",
    "    tokenizer.pad_token_id = 0\n",
    "    tokenizer.eos_token_id = 1\n",
    "    tokenizer.bos_token_id = 2\n",
    "\n",
    "    def tokenize(prompt, add_eos_token=True):\n",
    "        results = tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=max_seq_length,\n",
    "            padding=False,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        for i in range(len(results[\"input_ids\"])):\n",
    "            if (\n",
    "                results[\"input_ids\"][i][-1] != tokenizer.eos_token_id\n",
    "                and len(results[\"input_ids\"][i]) < max_seq_length\n",
    "                and add_eos_token\n",
    "            ):\n",
    "                results[\"input_ids\"][i].append(tokenizer.eos_token_id)\n",
    "                results[\"attention_mask\"][i].append(1)\n",
    "\n",
    "        results[\"labels\"] = copy.deepcopy(results[\"input_ids\"])\n",
    "        results[\"input_id_len\"] = [len(result) for result in results[\"input_ids\"]]\n",
    "        return results\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        keys = list(examples.data.keys())\n",
    "        if len(keys) != 2:\n",
    "            raise ValueError(\"Unsupported dataset format\")\n",
    "\n",
    "        st = [s + t for s, t in zip(examples[keys[0]], examples[keys[1]])]\n",
    "\n",
    "        examples_tokenized = tokenize(st)\n",
    "        input_ids = examples_tokenized[\"input_ids\"]\n",
    "        labels = examples_tokenized[\"labels\"]\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"labels\": labels,\n",
    "            \"attention_mask\": examples_tokenized[\"attention_mask\"],\n",
    "        }\n",
    "\n",
    "    tokenized_datasets = raw_datasets.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        load_from_cache_file=True,\n",
    "    )\n",
    "\n",
    "    def concatenate_data(dataset, max_seq_length):\n",
    "        concatenated_dataset = {}\n",
    "        for column in dataset.features:\n",
    "            concatenated_data = [item for sample in dataset[column] for item in sample]\n",
    "            reshaped_data = [\n",
    "                concatenated_data[i * max_seq_length : (i + 1) * max_seq_length]\n",
    "                for i in range(len(concatenated_data) // max_seq_length)\n",
    "            ]\n",
    "            concatenated_dataset[column] = reshaped_data\n",
    "        return datasets.Dataset.from_dict(concatenated_dataset)\n",
    "\n",
    "    tokenized_datasets_ = tokenized_datasets[\"train\"].remove_columns([\"prompt_sources\", \"prompt_targets\"])\n",
    "    tokenized_datasets[\"train\"] = concatenate_data(tokenized_datasets_, max_seq_length)\n",
    "\n",
    "    return tokenized_datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataloader Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def prepare_dataloader(datasets, tokenizer):\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", mlm=False)\n",
    "    print(f\"Using data collator of type {data_collator.__class__.__name__}\")\n",
    "\n",
    "    train_dataloader_params = {\n",
    "        \"shuffle\": False,\n",
    "        \"collate_fn\": data_collator,\n",
    "        \"batch_size\": 8,\n",
    "        \"pin_memory\": True,\n",
    "    }\n",
    "    train_dataset = datasets[\"train\"]\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, **train_dataloader_params)\n",
    "    return train_dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_func_per_worker(config: Dict):\n",
    "    # prepare datasets\n",
    "    raw_datasets = load_dataset(\"tatsu-lab/alpaca\")\n",
    "    preprocess_dataset(raw_datasets)\n",
    "\n",
    "    # prepare tokenizer\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(config[\"model\"])\n",
    "    tokenized_datasets = preprocess_dataset_to_tokenizer(raw_datasets, tokenizer)\n",
    "\n",
    "    # prepare dataloader\n",
    "    train_dataloader = prepare_dataloader(tokenized_datasets, tokenizer)\n",
    "\n",
    "    # prepare model\n",
    "    model = transformers.AutoModelForCausalLM.from_pretrained(config[\"model\"], **config[\"model_config\"])\n",
    "    peft_config = peft.LoraConfig(**config[\"lora_config\"])\n",
    "    model = peft.get_peft_model(model, peft_config)\n",
    "    device = ray.train.torch.get_device()\n",
    "    model.to(dtype=config[\"model_config\"][\"torch_dtype\"], device=device)\n",
    "\n",
    "    # prepare optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    print(f\"device = {device}, config = {config}\")\n",
    "\n",
    "    # create accelerator\n",
    "    accelerator = GaudiAccelerator()\n",
    "    accelerator.wait_for_everyone()\n",
    "    steps_per_epoch = len(train_dataloader)\n",
    "    num_train_epoch = config[\"epochs\"]\n",
    "    max_train_steps = num_train_epoch * steps_per_epoch\n",
    "    print(f\"num_train_epoch = {num_train_epoch}, max_train_steps = {max_train_steps}\")\n",
    "    lr_scheduler = transformers.get_scheduler(name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=max_train_steps)\n",
    "    model.train()\n",
    "    if config[\"execution_mode\"] == \"eager.compile\":\n",
    "        model = torch.compile(model,backend=\"hpu_backend\")\n",
    "    model = accelerator.prepare(model)\n",
    "    optimizer, train_dataloader, lr_scheduler = accelerator.prepare(optimizer, train_dataloader, lr_scheduler)\n",
    "\n",
    "    # training\n",
    "    logging_steps = 1\n",
    "    for epoch in range(num_train_epoch):\n",
    "        # train one epoch here\n",
    "        start = time.time()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            with accelerator.accumulate(model):\n",
    "                model.train()\n",
    "                batch = batch.to(device=device)\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "                accelerator.backward(loss)\n",
    "                htcore.mark_step()\n",
    "                optimizer.step()\n",
    "                htcore.mark_step()\n",
    "                lr_scheduler.step()\n",
    "                htcore.mark_step()\n",
    "                optimizer.zero_grad()\n",
    "                if step % logging_steps == 0:\n",
    "                    loss = loss.item()\n",
    "                    epochs = epoch + step / steps_per_epoch\n",
    "                    elapsed_time = time.time() - start\n",
    "                    print(f\"train epoch: {epochs:.6f}\\tloss:{loss:.6f}\\ttime:{elapsed_time:.6f}\")\n",
    "                    start = time.time()\n",
    "        # evaluate here\n",
    "        # model.eval()\n",
    "\n",
    "        # save checkpoint here\n",
    "        # torch.save(...)\n",
    "\n",
    "        accelerator.wait_for_everyone()\n",
    "\n",
    "    # save model\n",
    "    output = config[\"output\"]\n",
    "    print(f\"start save model to {output}\")\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output, is_main_process=accelerator.is_main_process, save_function=accelerator.save)\n",
    "    print(f\"finish save model to {output}\")\n",
    "    accelerator.wait_for_everyone()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Training Function\n",
    "The `train_llama` function sets up the distributed training environment using Ray and starts the training process. To enable training using HPU, we only need to make the following changes:\n",
    "- Set the exectuion mode for training, supported execution mode are:\n",
    "\n",
    "    - \"lazy\": Deferred execution of graphs, comprising of ops delivered from script op by op similar to Eager mode. It gives the Eager mode experience with performance on Gaudi. Unlike Eager Mode with torch.compile, graph is analyzed in each iteration leading to a higher CPU usage.\n",
    "    - \"eager\": Op-by-op execution as defined in standard PyTorch Eager mode scripts.\n",
    "    - \"eager.compile\": Eager mode extended with `torch.compile` - Similar to Eager mode but extended with wrapping complete or part of model (such as a function) into a graph. Parts that are not wrapped are executed eagerly.\n",
    "\n",
    "    More detail theory can be found [here](https://docs.habana.ai/en/latest/PyTorch/Reference/PyTorch_Gaudi_Theory_of_Operations.html)\n",
    "- Require an HPU for each worker in ScalingConfig\n",
    "- Set backend to \"hccl\" in TorchConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_llama(num_workers=2, execution_mode=\"lazy\"):\n",
    "    # Setting environment variables\n",
    "    os.environ[\"RAY_EXPERIMENTAL_NOSET_HABANA_VISIBLE_MODULES\"] = \"true\"\n",
    "    if execution_mode == \"lazy\":\n",
    "        os.environ[\"PT_HPU_LAZY_MODE\"] = \"1\"\n",
    "    else:\n",
    "        os.environ[\"PT_HPU_LAZY_MODE\"] = \"0\"\n",
    "\n",
    "    # Preparing train configurations\n",
    "    train_config = {\n",
    "        \"model\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "        \"model_config\": {\"torch_dtype\": torch.bfloat16, \"trust_remote_code\": False, \"use_auth_token\": None},\n",
    "        \"lora_config\": {\"task_type\": \"CAUSAL_LM\", \"r\": 8, \"lora_alpha\": 32, \"lora_dropout\": 0.1, \"target_modules\": [\"q_proj\", \"v_proj\"]},\n",
    "        \"lr\": 1e-3,\n",
    "        \"epochs\": 2,\n",
    "        \"batch_size_per_worker\": 8,\n",
    "        \"output\": \"/tmp/ray/\",\n",
    "    }\n",
    "\n",
    "    # Configure computation resources\n",
    "    # In ScalingConfig, require an HPU for each worker\n",
    "    scaling_config = ScalingConfig(num_workers=num_workers, resources_per_worker={\"CPU\": 1, \"HPU\": 1})\n",
    "    # Set backend to hccl in TorchConfig\n",
    "    torch_config = TorchConfig(backend = \"hccl\")\n",
    "\n",
    "    # start your ray cluster\n",
    "    ray.init()\n",
    "\n",
    "    # Initialize a Ray TorchTrainer\n",
    "    trainer = TorchTrainer(\n",
    "        train_loop_per_worker=train_func_per_worker,\n",
    "        train_loop_config=train_config,\n",
    "        torch_config=torch_config,\n",
    "        scaling_config=scaling_config,\n",
    "    )\n",
    "\n",
    "    result = trainer.fit()\n",
    "    print(f\"Training result: {result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training\n",
    "\n",
    "Finally, we call the `train_llama` function to start the training process. You can adjust the number of workers to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# execution_model are [\"lazy\", \"eager\", \"eager.compile\"]\n",
    "train_llama(num_workers=8, execution_mode=\"lazy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "(TorchTrainer pid=283433) Started distributed worker processes: \n",
    "(TorchTrainer pid=283433) - (ip=100.83.111.248, pid=284079) world_rank=0, local_rank=0, node_rank=0\n",
    "(TorchTrainer pid=283433) - (ip=100.83.111.248, pid=284080) world_rank=1, local_rank=1, node_rank=0\n",
    "(TorchTrainer pid=283433) - (ip=100.83.111.248, pid=284081) world_rank=2, local_rank=2, node_rank=0\n",
    "(TorchTrainer pid=283433) - (ip=100.83.111.248, pid=284082) world_rank=3, local_rank=3, node_rank=0\n",
    "(TorchTrainer pid=283433) - (ip=100.83.111.248, pid=284083) world_rank=4, local_rank=4, node_rank=0\n",
    "(TorchTrainer pid=283433) - (ip=100.83.111.248, pid=284084) world_rank=5, local_rank=5, node_rank=0\n",
    "(TorchTrainer pid=283433) - (ip=100.83.111.248, pid=284085) world_rank=6, local_rank=6, node_rank=0\n",
    "(TorchTrainer pid=283433) - (ip=100.83.111.248, pid=284086) world_rank=7, local_rank=7, node_rank=0\n",
    "\n",
    "...\n",
    "\n",
    "(RayTrainWorker pid=284079) ============================= HABANA PT BRIDGE CONFIGURATION =========================== \n",
    "(RayTrainWorker pid=284079)  PT_HPU_LAZY_MODE = 1\n",
    "(RayTrainWorker pid=284079)  PT_RECIPE_CACHE_PATH = \n",
    "(RayTrainWorker pid=284079)  PT_CACHE_FOLDER_DELETE = 0\n",
    "(RayTrainWorker pid=284079)  PT_HPU_RECIPE_CACHE_CONFIG = \n",
    "(RayTrainWorker pid=284079)  PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
    "(RayTrainWorker pid=284079)  PT_HPU_LAZY_ACC_PAR_MODE = 1\n",
    "(RayTrainWorker pid=284079)  PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
    "(RayTrainWorker pid=284079) ---------------------------: System Configuration :---------------------------\n",
    "(RayTrainWorker pid=284079) Num CPU Cores : 152\n",
    "(RayTrainWorker pid=284079) CPU RAM       : 1056440372 KB\n",
    "(RayTrainWorker pid=284079) ------------------------------------------------------------------------------\n",
    "\n",
    "...\n",
    "\n",
    "Training started with configuration:\n",
    "╭─────────────────────────────────────────────────────────────────────────╮\n",
    "│ Training config                                                         │\n",
    "├─────────────────────────────────────────────────────────────────────────┤\n",
    "│ train_loop_config/batch_size_per_worker                               8 │\n",
    "│ train_loop_config/epochs                                              2 │\n",
    "│ train_loop_config/lora_config/lora_alpha                             32 │\n",
    "│ train_loop_config/lora_config/lora_dropout                          0.1 │\n",
    "│ train_loop_config/lora_config/r                                       8 │\n",
    "│ train_loop_config/lora_config/target_modules       ['q_proj', 'v_proj'] │\n",
    "│ train_loop_config/lora_config/task_type                       CAUSAL_LM │\n",
    "│ train_loop_config/lr                                              0.001 │\n",
    "│ train_loop_config/model                            ...cfe6329f31193e33/ │\n",
    "│ train_loop_config/model_config/torch_dtype               torch.bfloat16 │\n",
    "│ train_loop_config/model_config/trust_remote_code                  False │\n",
    "│ train_loop_config/model_config/use_auth_token                           │\n",
    "│ train_loop_config/output                                      /tmp/ray/ │\n",
    "╰─────────────────────────────────────────────────────────────────────────╯\n",
    "\n",
    "...\n",
    "\n",
    "(RayTrainWorker pid=284086) train epoch: 0.000000       loss:1.810490   time:50.370564 [repeated 6x across cluster]\n",
    "(RayTrainWorker pid=284079) train epoch: 0.000620       loss:1.601695   time:28.551455 [repeated 2x across cluster]\n",
    "(RayTrainWorker pid=284084) train epoch: 0.006824       loss:1.052457   time:0.452178 [repeated 80x across cluster]\n",
    "(RayTrainWorker pid=284084) train epoch: 0.013648       loss:0.927009   time:0.500558 [repeated 88x across cluster]\n",
    "(RayTrainWorker pid=284084) train epoch: 0.021092       loss:0.948904   time:0.458447 [repeated 96x across cluster]\n",
    "(RayTrainWorker pid=284084) train epoch: 0.027916       loss:1.138537   time:0.450806 [repeated 88x across cluster]\n",
    "(RayTrainWorker pid=284084) train epoch: 0.034739       loss:0.963900   time:0.454531 [repeated 88x across cluster]\n",
    "(RayTrainWorker pid=284084) train epoch: 0.042184       loss:0.882160   time:0.452009 [repeated 96x across cluster]\n",
    "(RayTrainWorker pid=284084) train epoch: 0.049628       loss:0.873736   time:0.582011 [repeated 96x across cluster]\n",
    "(RayTrainWorker pid=284084) train epoch: 0.056452       loss:0.889894   time:0.452172 [repeated 88x across cluster]\n",
    "(RayTrainWorker pid=284084) train epoch: 0.063275       loss:0.896917   time:0.452242 [repeated 88x across cluster]\n",
    "(RayTrainWorker pid=284084) train epoch: 0.070720       loss:0.967623   time:0.452712 [repeated 96x across cluster]\n",
    "(RayTrainWorker pid=284084) train epoch: 0.078164       loss:0.820346   time:0.450104 [repeated 96x across cluster]\n",
    "(RayTrainWorker pid=284086) train epoch: 0.084988       loss:0.926922   time:0.654745 [repeated 92x across cluster]\n",
    "(RayTrainWorker pid=284084) train epoch: 0.092432       loss:0.934870   time:0.447995 [repeated 92x across cluster]\n",
    "(RayTrainWorker pid=284084) train epoch: 0.099876       loss:0.953212   time:0.451585 [repeated 96x across cluster]\n",
    "(RayTrainWorker pid=284084) train epoch: 0.106700       loss:0.801883   time:0.447374 [repeated 88x across cluster]\n",
    "(RayTrainWorker pid=284079) train epoch: 0.114144       loss:1.025253   time:0.448342 [repeated 96x across cluster]\n",
    "(RayTrainWorker pid=284084) train epoch: 0.120968       loss:0.888073   time:0.448579 [repeated 88x across cluster]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
