{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Llama-2 Model with HPU\n",
    "\n",
    "In this Jupyter notebook, we will fine-tune a [Llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) model by using HPU in DDP accelerate mode. We will use PyTorch for model training and Ray for distributed training. We will use dataset [tatsu-lab/alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca).\n",
    "\n",
    "[Habana Gaudi AI Processors (HPUs)](https://habana.ai) are AI hardware accelerators designed by Habana Labs. For more information, see [Gaudi Architecture](https://docs.habana.ai/en/latest/Gaudi_Overview/index.html) and [Gaudi Developer Docs](https://developer.habana.ai/).\n",
    "\n",
    "Basic features for this fine-tuning example are:\n",
    "- Running on HPUs, support three execution mode: \"lazy\", \"eager\", \"eager.compile\".\n",
    "- LoRA training.\n",
    "- `accelerate` based training.\n",
    "- Llama-2-7b model.\n",
    "- Ray based scheduling and management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare environment\n",
    "A node with Gaudi/Gaudi2 installed is required to run this example. This example will use 4 workers to train the model, each using 1 HPU.\n",
    "\n",
    "We recommend using a prebuilt container to run these examples. To run a container, you need Docker. See [Install Docker Engine](https://docs.docker.com/engine/install/) for installation instructions.\n",
    "\n",
    "Next, follow [Run Using Containers](https://docs.habana.ai/en/latest/Installation_Guide/Bare_Metal_Fresh_OS.html?highlight=installer#run-using-containers) to install the Habana drivers and container runtime.\n",
    "\n",
    "### Get docker image\n",
    "``` bash\n",
    "docker pull vault.habana.ai/gaudi-docker/1.15.1/ubuntu22.04/habanalabs/pytorch-installer-2.2.0:latest\n",
    "```\n",
    "### Run docker image\n",
    "``` bash\n",
    "docker run -it --runtime=habana -e HABANA_VISIBLE_DEVICES=all -e OMPI_MCA_btl_vader_single_copy_mechanism=none --cap-add=sys_nice --net=host --ipc=host vault.habana.ai/gaudi-docker/1.15.1/ubuntu22.04/habanalabs/pytorch-installer-2.2.0:latest\n",
    "# maybe should mapping your workspace volumns\n",
    "```\n",
    "### Install dependency\n",
    "``` bash\n",
    "pip install ray[train] notebook transformers datasets evaluate peft accelerate optimum-habana\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:319: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "2024-04-11 08:07:37,694\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2024-04-11 08:07:37,760\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2024-04-11 08:07:37,919\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import time\n",
    "from typing import Dict\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    AutoModelForSequenceClassification,\n",
    ")\n",
    "\n",
    "import peft\n",
    "\n",
    "import ray.train\n",
    "from ray.train import ScalingConfig\n",
    "from ray.train.torch import TorchTrainer\n",
    "from ray.train.torch import TorchConfig\n",
    "\n",
    "import habana_frameworks.torch.core as htcore\n",
    "\n",
    "from optimum.habana.accelerate import GaudiAccelerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset Function\n",
    "\n",
    "Preprocessing the raw dataset's each line with specified format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_dataset(raw_datasets):\n",
    "\n",
    "    PROMPT_DICT = {\n",
    "        \"prompt_with_input\": (\n",
    "            \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "            \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n",
    "        ),\n",
    "        \"prompt_without_input\": (\n",
    "            \"Below is an instruction that describes a task. \"\n",
    "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "            \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    def create_prompts(examples):\n",
    "        prompts = {}\n",
    "        prompts[\"source\"] = []\n",
    "        prompts[\"target\"] = []\n",
    "        for example in examples:\n",
    "            prompt_template = (\n",
    "                PROMPT_DICT[\"prompt_with_input\"] if example[\"input\"] != \"\" else PROMPT_DICT[\"prompt_without_input\"]\n",
    "            )\n",
    "            source = prompt_template.format_map(example)\n",
    "            prompts[\"source\"].append(source)\n",
    "            prompts[\"target\"].append(example[\"output\"])\n",
    "        return prompts\n",
    "\n",
    "    # Preprocessing the datasets.\n",
    "    for key in raw_datasets:\n",
    "        prompts = create_prompts(raw_datasets[key])\n",
    "        columns_to_be_removed = list(raw_datasets[key].features.keys())\n",
    "        raw_datasets[key] = raw_datasets[key].add_column(\"prompt_sources\", prompts[\"source\"])\n",
    "        raw_datasets[key] = raw_datasets[key].add_column(\"prompt_targets\", prompts[\"target\"])\n",
    "        raw_datasets[key] = raw_datasets[key].remove_columns(columns_to_be_removed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset to Tokenizer Function\n",
    "\n",
    "Tokenize each line in dataset by model tokenizer.\n",
    "\n",
    "In example codes, we concatenate the dataset's line content to accelerate training speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_dataset_to_tokenizer(raw_datasets, tokenizer):\n",
    "    max_seq_length = 512\n",
    "    tokenizer.pad_token_id = 0\n",
    "    tokenizer.eos_token_id = 1\n",
    "    tokenizer.bos_token_id = 2\n",
    "\n",
    "    def tokenize(prompt, add_eos_token=True):\n",
    "        results = tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=max_seq_length,\n",
    "            padding=False,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        for i in range(len(results[\"input_ids\"])):\n",
    "            if (\n",
    "                results[\"input_ids\"][i][-1] != tokenizer.eos_token_id\n",
    "                and len(results[\"input_ids\"][i]) < max_seq_length\n",
    "                and add_eos_token\n",
    "            ):\n",
    "                results[\"input_ids\"][i].append(tokenizer.eos_token_id)\n",
    "                results[\"attention_mask\"][i].append(1)\n",
    "\n",
    "        results[\"labels\"] = copy.deepcopy(results[\"input_ids\"])\n",
    "        results[\"input_id_len\"] = [len(result) for result in results[\"input_ids\"]]\n",
    "        return results\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        keys = list(examples.data.keys())\n",
    "        if len(keys) != 2:\n",
    "            raise ValueError(\"Unsupported dataset format\")\n",
    "\n",
    "        st = [s + t for s, t in zip(examples[keys[0]], examples[keys[1]])]\n",
    "\n",
    "        examples_tokenized = tokenize(st)\n",
    "        input_ids = examples_tokenized[\"input_ids\"]\n",
    "        labels = examples_tokenized[\"labels\"]\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"labels\": labels,\n",
    "            \"attention_mask\": examples_tokenized[\"attention_mask\"],\n",
    "        }\n",
    "\n",
    "    tokenized_datasets = raw_datasets.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        load_from_cache_file=True,\n",
    "    )\n",
    "\n",
    "    def concatenate_data(dataset, max_seq_length):\n",
    "        concatenated_dataset = {}\n",
    "        for column in dataset.features:\n",
    "            concatenated_data = [item for sample in dataset[column] for item in sample]\n",
    "            reshaped_data = [\n",
    "                concatenated_data[i * max_seq_length : (i + 1) * max_seq_length]\n",
    "                for i in range(len(concatenated_data) // max_seq_length)\n",
    "            ]\n",
    "            concatenated_dataset[column] = reshaped_data\n",
    "        return datasets.Dataset.from_dict(concatenated_dataset)\n",
    "\n",
    "    tokenized_datasets_ = tokenized_datasets[\"train\"].remove_columns([\"prompt_sources\", \"prompt_targets\"])\n",
    "    tokenized_datasets[\"train\"] = concatenate_data(tokenized_datasets_, max_seq_length)\n",
    "\n",
    "    return tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataloader Function\n",
    "\n",
    "Convert tokenized dataset to dataloader by using `DataCollatorForLanguageModeling` in transformers.\n",
    "\n",
    "No need to provide evaluation dataset, the example doesn't support evaluation for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_dataloader(datasets, tokenizer):\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", mlm=False)\n",
    "    print(f\"Using data collator of type {data_collator.__class__.__name__}\")\n",
    "\n",
    "    train_dataloader_params = {\n",
    "        \"shuffle\": False,\n",
    "        \"collate_fn\": data_collator,\n",
    "        \"batch_size\": 8,\n",
    "        \"pin_memory\": True,\n",
    "    }\n",
    "    train_dataset = datasets[\"train\"]\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, **train_dataloader_params)\n",
    "    return train_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function\n",
    "\n",
    "This function will be executed by each worker during training, with following steps:\n",
    "\n",
    "- loading datasets and preprocess datasets.\n",
    "- loading pretrained model as tokenizer, and process datasets to tokenizer.\n",
    "- loading pretrained model, convert to lora model, and move model to HPU device.\n",
    "- creating optimizer.\n",
    "- creating `GaudiAccelerator` instance.\n",
    "- executing training loop.\n",
    "- saving the fine-tuned model.\n",
    "\n",
    "Compared to transformers `Trainer` use `Accelerator` for training models,\n",
    "here example codes use `GaudiAccelerator` to make training on distributed environment more simple, efficient and adaptable.\n",
    "\n",
    "Compared to a training function for GPU, no changes are needed to port to HPU. Internally, Ray Train does these things:\n",
    "\n",
    "- Detect HPU and set the device.\n",
    "- Initialize the habana PyTorch backend.\n",
    "- Initialize the habana distributed backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_func_per_worker(config: Dict):\n",
    "    # prepare datasets\n",
    "    raw_datasets = load_dataset(\"tatsu-lab/alpaca\")\n",
    "    preprocess_dataset(raw_datasets)\n",
    "\n",
    "    # prepare tokenizer\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(config[\"model\"])\n",
    "    tokenized_datasets = preprocess_dataset_to_tokenizer(raw_datasets, tokenizer)\n",
    "\n",
    "    # prepare dataloader\n",
    "    train_dataloader = prepare_dataloader(tokenized_datasets, tokenizer)\n",
    "\n",
    "    # prepare model\n",
    "    model = transformers.AutoModelForCausalLM.from_pretrained(config[\"model\"], **config[\"model_config\"])\n",
    "    peft_config = peft.LoraConfig(**config[\"lora_config\"])\n",
    "    model = peft.get_peft_model(model, peft_config)\n",
    "    device = ray.train.torch.get_device()\n",
    "    model.to(dtype=config[\"model_config\"][\"torch_dtype\"], device=device)\n",
    "\n",
    "    # prepare optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    print(f\"device = {device}, config = {config}\")\n",
    "\n",
    "    # create accelerator\n",
    "    accelerator = GaudiAccelerator()\n",
    "    accelerator.wait_for_everyone()\n",
    "    steps_per_epoch = len(train_dataloader)\n",
    "    num_train_epoch = config[\"epochs\"]\n",
    "    max_train_steps = num_train_epoch * steps_per_epoch\n",
    "    print(f\"num_train_epoch = {num_train_epoch}, max_train_steps = {max_train_steps}\")\n",
    "    lr_scheduler = transformers.get_scheduler(name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=max_train_steps)\n",
    "    model.train()\n",
    "    if config[\"execution_mode\"] == \"eager.compile\":\n",
    "        model = torch.compile(model,backend=\"hpu_backend\")\n",
    "    model = accelerator.prepare(model)\n",
    "    optimizer, train_dataloader, lr_scheduler = accelerator.prepare(optimizer, train_dataloader, lr_scheduler)\n",
    "\n",
    "    # training\n",
    "    logging_steps = 1\n",
    "    for epoch in range(num_train_epoch):\n",
    "        # train one epoch here\n",
    "        start = time.time()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            with accelerator.accumulate(model):\n",
    "                model.train()\n",
    "                batch = batch.to(device=device)\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "                accelerator.backward(loss)\n",
    "                htcore.mark_step()\n",
    "                optimizer.step()\n",
    "                htcore.mark_step()\n",
    "                lr_scheduler.step()\n",
    "                htcore.mark_step()\n",
    "                optimizer.zero_grad()\n",
    "                if step % logging_steps == 0:\n",
    "                    loss = loss.item()\n",
    "                    epochs = epoch + step / steps_per_epoch\n",
    "                    elapsed_time = time.time() - start\n",
    "                    print(f\"train epoch: {epochs:.6f}\\tloss:{loss:.6f}\\ttime:{elapsed_time:.6f}\")\n",
    "                    start = time.time()\n",
    "        # evaluate here\n",
    "        # model.eval()\n",
    "\n",
    "        # save checkpoint here\n",
    "        # torch.save(...)\n",
    "\n",
    "        accelerator.wait_for_everyone()\n",
    "\n",
    "    # save model\n",
    "    output = config[\"output\"]\n",
    "    print(f\"start save model to {output}\")\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output, is_main_process=accelerator.is_main_process, save_function=accelerator.save)\n",
    "    print(f\"finish save model to {output}\")\n",
    "    accelerator.wait_for_everyone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Training Function\n",
    "The `train_llama` function sets up the distributed training environment using Ray and starts the training process. To enable training using HPU, we only need to make the following changes:\n",
    "- Set the exectuion mode for training, supported execution mode are:\n",
    "\n",
    "    - \"lazy\": Deferred execution of graphs, comprising of ops delivered from script op by op similar to Eager mode. It gives the Eager mode experience with performance on Gaudi. Unlike Eager Mode with torch.compile, graph is analyzed in each iteration leading to a higher CPU usage.\n",
    "    - \"eager\": Op-by-op execution as defined in standard PyTorch Eager mode scripts.\n",
    "    - \"eager.compile\": Eager mode extended with `torch.compile` - Similar to Eager mode but extended with wrapping complete or part of model (such as a function) into a graph. Parts that are not wrapped are executed eagerly.\n",
    "\n",
    "    More detail theory can be found [here](https://docs.habana.ai/en/latest/PyTorch/Reference/PyTorch_Gaudi_Theory_of_Operations.html)\n",
    "- Require an HPU for each worker in ScalingConfig\n",
    "- Set backend to `hccl` in TorchConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_llama(num_workers=2, execution_mode=\"lazy\"):\n",
    "    # Setting environment variables\n",
    "    os.environ[\"RAY_EXPERIMENTAL_NOSET_HABANA_VISIBLE_MODULES\"] = \"true\"\n",
    "    if execution_mode == \"lazy\":\n",
    "        os.environ[\"PT_HPU_LAZY_MODE\"] = \"1\"\n",
    "    else:\n",
    "        os.environ[\"PT_HPU_LAZY_MODE\"] = \"0\"\n",
    "\n",
    "    # Preparing train configurations\n",
    "    train_config = {\n",
    "        \"execution_mode\": execution_mode,\n",
    "        \"model\": \"/root/models/llama-7b\",\n",
    "        \"model_config\": {\"torch_dtype\": torch.bfloat16, \"trust_remote_code\": False, \"use_auth_token\": None},\n",
    "        \"lora_config\": {\"task_type\": \"CAUSAL_LM\", \"r\": 8, \"lora_alpha\": 32, \"lora_dropout\": 0.1, \"target_modules\": [\"q_proj\", \"v_proj\"]},\n",
    "        \"lr\": 1e-4,\n",
    "        \"epochs\": 2,\n",
    "        \"batch_size_per_worker\": 8,\n",
    "        \"output\": \"/tmp/ray/\",\n",
    "    }\n",
    "\n",
    "    # Configure computation resources\n",
    "    # In ScalingConfig, require an HPU for each worker\n",
    "    scaling_config = ScalingConfig(num_workers=num_workers, resources_per_worker={\"CPU\": 1, \"HPU\": 1})\n",
    "    # Set backend to hccl in TorchConfig\n",
    "    torch_config = TorchConfig(backend = \"hccl\")\n",
    "\n",
    "    # start your ray cluster\n",
    "    ray.init()\n",
    "\n",
    "    # Initialize a Ray TorchTrainer\n",
    "    trainer = TorchTrainer(\n",
    "        train_loop_per_worker=train_func_per_worker,\n",
    "        train_loop_config=train_config,\n",
    "        torch_config=torch_config,\n",
    "        scaling_config=scaling_config,\n",
    "    )\n",
    "\n",
    "    result = trainer.fit()\n",
    "    print(f\"Training result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training\n",
    "\n",
    "Finally, we call the `train_llama` function to start the training process. You can adjust the number of workers to use, and the execution mode for HPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-04-11 08:18:16</td></tr>\n",
       "<tr><td>Running for: </td><td>00:09:23.44        </td></tr>\n",
       "<tr><td>Memory:      </td><td>97.3/1007.4 GiB    </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 5.0/160 CPUs, 0/0 GPUs (0.0/1.0 TPU, 4.0/8.0 HPU)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status    </th><th>loc              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_bca23_00000</td><td>TERMINATED</td><td>10.7.4.144:152049</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=152049)\u001b[0m /usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "\u001b[36m(pid=152049)\u001b[0m   _torch_pytree._register_pytree_node(\n",
      "\u001b[36m(TrainTrainable pid=152049)\u001b[0m /usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:319: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "\u001b[36m(TrainTrainable pid=152049)\u001b[0m   _torch_pytree._register_pytree_node(\n",
      "\u001b[36m(RayTrainWorker pid=152619)\u001b[0m /usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "\u001b[36m(RayTrainWorker pid=152619)\u001b[0m   _torch_pytree._register_pytree_node(\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m /usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m   _torch_pytree._register_pytree_node(\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m Setting up process group for: env:// [rank=0, world_size=4]\n",
      "\u001b[36m(TorchTrainer pid=152049)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=152049)\u001b[0m - (ip=10.7.4.144, pid=152616) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=152049)\u001b[0m - (ip=10.7.4.144, pid=152617) world_rank=1, local_rank=1, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=152049)\u001b[0m - (ip=10.7.4.144, pid=152618) world_rank=2, local_rank=2, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=152049)\u001b[0m - (ip=10.7.4.144, pid=152619) world_rank=3, local_rank=3, node_rank=0\n",
      "Map:   0%|          | 0/52002 [00:00<?, ? examples/s]\n",
      "\u001b[36m(RayTrainWorker pid=152618)\u001b[0m /usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:319: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\u001b[32m [repeated 6x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152618)\u001b[0m   _torch_pytree._register_pytree_node(\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Map:   2%|▏         | 1000/52002 [00:00<00:21, 2342.59 examples/s]\n",
      "Map:   0%|          | 0/52002 [00:00<?, ? examples/s]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Map:  50%|████▉     | 26000/52002 [00:05<00:04, 5233.82 examples/s]\u001b[32m [repeated 94x across cluster]\u001b[0m\n",
      "Map:  92%|█████████▏| 48000/52002 [00:09<00:00, 5305.72 examples/s]\n",
      "Map: 100%|██████████| 52002/52002 [00:10<00:00, 4968.12 examples/s]\n",
      "Map:  83%|████████▎ | 43000/52002 [00:08<00:01, 5142.96 examples/s]\u001b[32m [repeated 89x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=152618)\u001b[0m Using data collator of type DataCollatorForLanguageModeling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Map:  98%|█████████▊| 51000/52002 [00:10<00:00, 5425.65 examples/s]\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "Map: 100%|██████████| 52002/52002 [00:10<00:00, 4870.22 examples/s]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Map:  90%|█████████ | 47000/52002 [00:09<00:00, 5162.04 examples/s]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.81s/it]\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.32s/it]\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.67s/it]\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=152618)\u001b[0m device = hpu, config = {'execution_mode': 'lazy', 'model': '/root/models/llama-7b', 'model_config': {'torch_dtype': torch.bfloat16, 'trust_remote_code': False, 'use_auth_token': None}, 'lora_config': {'task_type': 'CAUSAL_LM', 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1, 'target_modules': ['q_proj', 'v_proj']}, 'lr': 0.0001, 'epochs': 2, 'batch_size_per_worker': 8, 'output': '/tmp/ray/'}\n",
      "\u001b[36m(RayTrainWorker pid=152618)\u001b[0m num_train_epoch = 2, max_train_steps = 3224\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m Using data collator of type DataCollatorForLanguageModeling\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m ============================= HABANA PT BRIDGE CONFIGURATION =========================== \n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m  PT_HPU_LAZY_MODE = 1\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m  PT_RECIPE_CACHE_PATH = \n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m  PT_CACHE_FOLDER_DELETE = 0\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m  PT_HPU_RECIPE_CACHE_CONFIG = \n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m  PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m  PT_HPU_LAZY_ACC_PAR_MODE = 1\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m  PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m ---------------------------: System Configuration :---------------------------\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m Num CPU Cores : 160\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m CPU RAM       : 1056375244 KB\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m ------------------------------------------------------------------------------\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.22s/it]\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=152618)\u001b[0m train epoch: 0.000000\tloss:2.170027\ttime:54.988902\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m device = hpu, config = {'execution_mode': 'lazy', 'model': '/root/models/llama-7b', 'model_config': {'torch_dtype': torch.bfloat16, 'trust_remote_code': False, 'use_auth_token': None}, 'lora_config': {'task_type': 'CAUSAL_LM', 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1, 'target_modules': ['q_proj', 'v_proj']}, 'lr': 0.0001, 'epochs': 2, 'batch_size_per_worker': 8, 'output': '/tmp/ray/'}\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m num_train_epoch = 2, max_train_steps = 3224\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152617)\u001b[0m train epoch: 0.000620\tloss:2.236194\ttime:32.765280\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 0.005583\tloss:1.603664\ttime:0.488202\u001b[32m [repeated 32x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 0.012407\tloss:1.465473\ttime:0.489304\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 0.018610\tloss:1.386875\ttime:0.491756\u001b[32m [repeated 40x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 0.025434\tloss:0.938031\ttime:0.485079\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152618)\u001b[0m train epoch: 0.032258\tloss:1.094995\ttime:0.614748\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 0.039082\tloss:0.957270\ttime:0.487000\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 0.045906\tloss:1.117040\ttime:0.488614\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152618)\u001b[0m train epoch: 0.052109\tloss:1.027171\ttime:0.487913\u001b[32m [repeated 40x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 0.058933\tloss:0.966696\ttime:0.487318\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 0.065757\tloss:0.872746\ttime:0.486151\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152618)\u001b[0m train epoch: 0.071960\tloss:1.060781\ttime:0.490972\u001b[32m [repeated 40x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 0.078784\tloss:1.093179\ttime:0.491578\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152618)\u001b[0m train epoch: 0.085608\tloss:1.044356\ttime:0.605468\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 0.091811\tloss:1.055124\ttime:0.493586\u001b[32m [repeated 40x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 0.098635\tloss:0.916802\ttime:0.499755\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 0.104839\tloss:0.981349\ttime:0.487636\u001b[32m [repeated 40x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 0.111663\tloss:0.816859\ttime:0.487065\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 0.118486\tloss:0.927926\ttime:0.499026\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 0.124690\tloss:0.842422\ttime:0.488993\u001b[32m [repeated 40x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 0.131514\tloss:0.971038\ttime:0.491688\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 0.138337\tloss:0.891899\ttime:0.489151\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 0.144541\tloss:0.940259\ttime:0.489195\u001b[32m [repeated 40x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 0.151365\tloss:0.977219\ttime:0.485064\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 0.157568\tloss:0.962215\ttime:0.488351\u001b[32m [repeated 40x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 0.164392\tloss:0.978222\ttime:0.488898\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 0.171216\tloss:0.976728\ttime:0.487358\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 0.177419\tloss:0.973859\ttime:0.490362\u001b[32m [repeated 40x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 0.184243\tloss:0.900707\ttime:0.498039\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 0.191067\tloss:0.955603\ttime:0.487731\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 0.197270\tloss:0.810532\ttime:0.488244\u001b[32m [repeated 40x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 0.204094\tloss:0.884896\ttime:0.490592\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152618)\u001b[0m train epoch: 0.210918\tloss:0.959227\ttime:0.486371\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 0.217122\tloss:0.927824\ttime:0.487361\u001b[32m [repeated 40x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 0.223945\tloss:0.876185\ttime:0.488048\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 0.230149\tloss:0.862724\ttime:0.630976\u001b[32m [repeated 40x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 0.236973\tloss:0.945926\ttime:0.493778\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 0.243797\tloss:0.911513\ttime:0.492378\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 1.000000\tloss:0.948796\ttime:0.496318\u001b[32m [repeated 40x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 1.006824\tloss:0.810773\ttime:0.493703\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 1.013648\tloss:0.814898\ttime:0.488539\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 1.019851\tloss:0.875827\ttime:0.486622\u001b[32m [repeated 40x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152618)\u001b[0m train epoch: 1.026675\tloss:0.856535\ttime:0.488379\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152618)\u001b[0m train epoch: 1.033499\tloss:1.046296\ttime:0.492180\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 1.039702\tloss:0.950364\ttime:0.488634\u001b[32m [repeated 40x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 1.046526\tloss:1.072678\ttime:0.488935\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 1.052730\tloss:0.970308\ttime:0.491720\u001b[32m [repeated 40x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 1.059553\tloss:0.894729\ttime:0.490652\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 1.066377\tloss:0.883815\ttime:0.492733\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 1.072581\tloss:0.811077\ttime:0.488775\u001b[32m [repeated 40x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 1.079404\tloss:0.752390\ttime:0.493634\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152618)\u001b[0m train epoch: 1.085608\tloss:1.006526\ttime:0.495755\u001b[32m [repeated 41x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 1.091811\tloss:1.019485\ttime:0.489071\u001b[32m [repeated 39x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 1.098635\tloss:0.881341\ttime:0.490731\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 1.105459\tloss:0.947351\ttime:0.490374\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 1.111663\tloss:0.787139\ttime:0.491069\u001b[32m [repeated 40x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 1.118486\tloss:0.896406\ttime:0.492208\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 1.124690\tloss:0.810354\ttime:0.519585\u001b[32m [repeated 40x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 1.131514\tloss:0.948413\ttime:0.488920\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 1.138337\tloss:0.870721\ttime:0.490399\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 1.144541\tloss:0.917201\ttime:0.491120\u001b[32m [repeated 40x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 1.151365\tloss:0.959115\ttime:0.490790\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 1.158189\tloss:0.921456\ttime:0.491004\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 1.164392\tloss:0.958567\ttime:0.492201\u001b[32m [repeated 40x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 1.170596\tloss:0.822486\ttime:0.487932\u001b[32m [repeated 40x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 1.177419\tloss:0.960204\ttime:0.488643\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 1.183623\tloss:0.920628\ttime:0.487433\u001b[32m [repeated 40x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 1.190447\tloss:0.946688\ttime:0.493605\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 1.197270\tloss:0.798931\ttime:0.491608\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 1.204094\tloss:0.872053\ttime:0.489729\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 1.210918\tloss:0.954754\ttime:0.489333\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 1.217742\tloss:0.895170\ttime:0.487065\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 1.224566\tloss:0.821548\ttime:0.486476\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 1.231390\tloss:0.986906\ttime:0.491562\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 1.237593\tloss:1.180848\ttime:0.486075\u001b[32m [repeated 40x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m train epoch: 1.244417\tloss:0.926204\ttime:0.488309\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=152616)\u001b[0m start save model to /tmp/ray/\n",
      "\u001b[36m(RayTrainWorker pid=152618)\u001b[0m finish save model to /tmp/ray/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=152618)\u001b[0m /usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /root/models/llama-7b - will assume that the vocabulary was not modified.\n",
      "\u001b[36m(RayTrainWorker pid=152618)\u001b[0m   warnings.warn(\n",
      "2024-04-11 08:18:16,938\tINFO tune.py:1021 -- Wrote the latest version of all result files and experiment state to '/root/ray_results/TorchTrainer_2024-04-11_08-08-53' in 0.0064s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial TorchTrainer_bca23_00000 completed. Last result: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-11 08:18:16,950\tINFO tune.py:1053 -- Total run time: 563.48 seconds (563.44 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training result: Result(\n",
      "  metrics={},\n",
      "  path='/root/ray_results/TorchTrainer_2024-04-11_08-08-53/TorchTrainer_bca23_00000_0_2024-04-11_08-08-53',\n",
      "  filesystem='local',\n",
      "  checkpoint=None\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# execution_mode are [\"lazy\", \"eager\", \"eager.compile\"]\n",
    "train_llama(num_workers=4, execution_mode=\"lazy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
