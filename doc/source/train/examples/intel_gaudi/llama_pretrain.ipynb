{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama model pre-training on HPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import os\n",
    "from typing import Any, Dict\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import transformers\n",
    "from transformers import HfArgumentParser, default_data_collator\n",
    "\n",
    "from megatron import get_args, print_rank_0\n",
    "from megatron.core import mpu\n",
    "from megatron.data import gpt_dataset\n",
    "from megatron.initialize import initialize_megatron\n",
    "from megatron.data.data_samplers import build_pretraining_data_loader\n",
    "from megatron.training import build_train_valid_test_datasets, update_train_iters\n",
    "\n",
    "from optimum.habana import GaudiConfig, GaudiTrainer, GaudiTrainingArguments\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare environment\n",
    "This example run on single node with 4 HPUs.\n",
    "\n",
    "We recommend using a prebuilt container to run these examples. To run a container, you need Docker. See [Install Docker Engine](https://docs.docker.com/engine/install/) for installation instructions.\n",
    "\n",
    "Next, follow [Run Using Containers](https://docs.habana.ai/en/latest/Installation_Guide/Bare_Metal_Fresh_OS.html?highlight=installer#run-using-containers) to install the Habana drivers and container runtime.\n",
    "\n",
    "### Get docker image\n",
    "``` bash\n",
    "docker pull vault.habana.ai/gaudi-docker/1.15.1/ubuntu22.04/habanalabs/pytorch-installer-2.2.0:latest\n",
    "```\n",
    "### Run docker image\n",
    "``` bash\n",
    "docker run -it --runtime=habana -e HABANA_VISIBLE_DEVICES=all -e OMPI_MCA_btl_vader_single_copy_mechanism=none --cap-add=sys_nice --net=host --ipc=host vault.habana.ai/gaudi-docker/1.15.1/ubuntu22.04/habanalabs/pytorch-installer-2.2.0:latest\n",
    "# maybe should mapping your workspace volumns\n",
    "```\n",
    "### Install dependency\n",
    "``` bash\n",
    "# \"optimum-habana>1.11.1\" if exection mode \"eager\" or \"eager.compile\" \n",
    "# \"ray>=2.20.0\"\n",
    "pip install ray[train] notebook transformers datasets evaluate peft accelerate scikit-learn optimum-habana\n",
    "\n",
    "# install deepspeed\n",
    "pip install git+https://github.com/HabanaAI/DeepSpeed.git@1.15.0\n",
    "\n",
    "# this notebook verfied with packages' version:\n",
    "# transformers==4.38.2\n",
    "# datasets==2.19.1\n",
    "# evaluate==0.4.2\n",
    "# peft==0.4.0\n",
    "# accelerate==0.27.2\n",
    "# scikit-learn==1.4.2\n",
    "# optimum-habana==1.11.1\n",
    "\n",
    "# deepspeed==0.12.4+hpu.synapse.v1.15.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MegatronDataset:\n",
    "    def __call__(self, config):\n",
    "        def _train_valid_test_datasets_provider(train_val_test_num_samples):\n",
    "            \"\"\"Build train, valid, and test datasets.\"\"\"\n",
    "            args = get_args()\n",
    "            print_rank_0(\"> building train, validation, and test datasets \" \"for GPT ...\")\n",
    "            train_ds, valid_ds, test_ds = gpt_dataset.build_train_valid_test_datasets(\n",
    "                data_prefix=args.data_path,\n",
    "                data_impl=args.data_impl,\n",
    "                splits_string=args.split,\n",
    "                train_valid_test_num_samples=train_val_test_num_samples,\n",
    "                seq_length=args.seq_length,\n",
    "                seed=args.seed,\n",
    "                skip_warmup=(not args.mmap_warmup),\n",
    "                train_data_prefix=args.train_data_path,\n",
    "                valid_data_prefix=args.valid_data_path,\n",
    "                test_data_prefix=args.test_data_path,\n",
    "                data_cache_path=args.data_cache_path,\n",
    "            )\n",
    "            print_rank_0(\"> finished creating GPT datasets ...\")\n",
    "\n",
    "            return train_ds, valid_ds, test_ds\n",
    "\n",
    "        args = get_args()\n",
    "        update_train_iters(args)\n",
    "        datasets = build_train_valid_test_datasets(_train_valid_test_datasets_provider)\n",
    "        print_rank_0(datasets)\n",
    "        return datasets\n",
    "\n",
    "\n",
    "def load_datasets(config):\n",
    "    dataset = MegatronDataset()\n",
    "    return dataset(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MegatronProcesser:\n",
    "    def prepare(self, tokenizer, dataset, **kwargs):\n",
    "        args = get_args()\n",
    "\n",
    "        (train_dataloader, valid_dataloader, test_dataloader) = (None, None, None)\n",
    "\n",
    "        print_rank_0(\"> building train, validation, and test datasets ...\")\n",
    "        iteration = kwargs.get(\"step\", 0)\n",
    "        if iteration:\n",
    "            # passed value is starting step\n",
    "            iteration -= 1\n",
    "            args.consumed_train_samples = iteration * args.global_batch_size\n",
    "            args.consumed_valid_samples = (\n",
    "                (args.iteration // args.eval_interval) * args.eval_iters * args.global_batch_size\n",
    "            )\n",
    "\n",
    "        # Data loader only on rank 0 of each model parallel group.\n",
    "        if args.use_dataset_only or mpu.get_tensor_model_parallel_rank() == 0:\n",
    "            # Build datasets.\n",
    "            train_ds, valid_ds, test_ds = dataset\n",
    "\n",
    "            # Build dataloders.\n",
    "            train_dataloader = build_pretraining_data_loader(train_ds, args.consumed_train_samples)\n",
    "            valid_dataloader = build_pretraining_data_loader(valid_ds, args.consumed_valid_samples)\n",
    "            test_dataloader = build_pretraining_data_loader(test_ds, 0)\n",
    "\n",
    "        return train_dataloader, valid_dataloader, test_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer(config):\n",
    "    name = config[\"name\"]\n",
    "    load_config = config[\"config\"]\n",
    "    return transformers.AutoTokenizer.from_pretrained(name, **load_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuggingFaceModelFromConfig:\n",
    "    def __call__(self, config):\n",
    "        name = config[\"name\"]\n",
    "        self.model_config = config.get(\"config\", {})\n",
    "        self.auto_config = None\n",
    "        if name is not None:\n",
    "            self.auto_config = transformers.AutoConfig.from_pretrained(\n",
    "                pretrained_model_name_or_path=name, **self.model_config\n",
    "            )\n",
    "        else:\n",
    "            self.auto_config = transformers.AutoConfig.for_model(**self.model_config)\n",
    "        self.model = transformers.AutoModelForCausalLM.from_config(self.auto_config)\n",
    "\n",
    "        return self.model\n",
    "\n",
    "\n",
    "def load_model(config):\n",
    "    model = HuggingFaceModelFromConfig()\n",
    "    return model(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HFCustomerSamplerTrainer(GaudiTrainer):  # type: ignore\n",
    "    def set_sampler(self, sampler):\n",
    "        self.customer_sampler = sampler\n",
    "\n",
    "    def get_train_dataloader(self) -> DataLoader:\n",
    "        if self.train_dataset is None:\n",
    "            raise ValueError(\"Trainer: training requires a train_dataset.\")\n",
    "\n",
    "        train_dataloader, _, _ = self.customer_sampler.prepare(\n",
    "            None, (self.train_dataset, None, None)\n",
    "        )\n",
    "        return train_dataloader\n",
    "\n",
    "\n",
    "def get_trainer(config, training_args, datasets, tokenizer, model):\n",
    "    gaudi_config = GaudiConfig.from_pretrained(\n",
    "        training_args.gaudi_config_name,\n",
    "        cache_dir=config.get(\"cache_dir\", None),\n",
    "        revision=config.get(\"model_revision\", None),\n",
    "        use_auth_token=True if config.get(\"use_auth_token\") else None,\n",
    "    )\n",
    "\n",
    "    train_dataset, eval_dataset, test_dataset = datasets\n",
    "\n",
    "    trainer = HFCustomerSamplerTrainer(\n",
    "        model=model,\n",
    "        gaudi_config=gaudi_config,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=None,\n",
    "        tokenizer=tokenizer,\n",
    "        # Data collator will default to DataCollatorWithPadding, so we change it.\n",
    "        data_collator=default_data_collator,\n",
    "        compute_metrics=None,\n",
    "        preprocess_logits_for_metrics=None\n",
    "    )\n",
    "    return trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_llama(config: Dict[str, Any]):\n",
    "\n",
    "    # initialize_megatron(config[\"megatron_config\"])\n",
    "    initialize_megatron(ignore_unknown_args=True, external_args=config[\"megatron_config\"], allow_no_cuda=True)\n",
    "\n",
    "    datasets = load_datasets(config[\"datasets\"])\n",
    "\n",
    "    dataprocessor = MegatronProcesser()\n",
    "\n",
    "    tokenizer = load_tokenizer(config[\"tokenizer\"])\n",
    "\n",
    "    training_args = GaudiTrainingArguments(**config[\"training_args\"])\n",
    "\n",
    "    model = load_model(config[\"model\"])\n",
    "\n",
    "    trainer = get_trainer(config, training_args, datasets, tokenizer, model)\n",
    "    trainer.set_sampler(dataprocessor)\n",
    "\n",
    "    result = trainer.train()\n",
    "    trainer.save_model()\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(num_workers):\n",
    "    import ray\n",
    "    from ray.train import ScalingConfig\n",
    "    from ray.train.torch import TorchTrainer, TorchConfig\n",
    "\n",
    "    pretrain_config = {\n",
    "        \"megatron_config\": {\n",
    "            \"data_path\": [\"/root/workspace/bigscience/data/oscar/zh/tokenized_text_document\"],\n",
    "            \"data_impl\": \"mmap\",\n",
    "            \"micro_batch_size\": 1,\n",
    "            \"global_batch_size\": 4,\n",
    "            \"seq_length\": 2048,\n",
    "            \"use_dataset_only\": True,\n",
    "            # \"vocab_file\": \"/home/user/workspace/data/gpt2-vocab.json\",\n",
    "            \"tokenizer_type\": \"HFTokenizer\",\n",
    "            \"tokenizer_model\": \"huggyllama/llama-7b\",\n",
    "            # \"merge_file\": \"/home/user/workspace/data/gpt2-merges.txt\",\n",
    "            \"eval_interval\": 1000,\n",
    "            \"train_samples\": 300_000_000,\n",
    "            \"split\": \"949,50,1\",\n",
    "        },\n",
    "        \"datasets\": {\n",
    "        },\n",
    "        \"tokenizer\": {\n",
    "            \"name\": \"huggyllama/llama-7b\",\n",
    "            \"config\": {}\n",
    "        },\n",
    "        \"model\": {\n",
    "            \"name\": \"huggyllama/llama-7b\",\n",
    "            \"config\": {\n",
    "                \"torch_dtype\": \"bfloat16\",\n",
    "            },\n",
    "        },\n",
    "        \"training_args\": {\n",
    "            \"per_device_train_batch_size\": 1,\n",
    "            \"per_device_eval_batch_size\": 1,\n",
    "            \"do_train\": True,\n",
    "            \"do_eval\": False,\n",
    "            \"save_strategy\": \"steps\",\n",
    "            \"save_steps\": 1000,\n",
    "            \"output_dir\": \"/tmp/pretrain-llama\",\n",
    "            \"gaudi_config_name\": \"Habana/gpt2\",\n",
    "            \"use_habana\": True,\n",
    "            \"max_steps\": 100000,\n",
    "            \"throughput_warmup_steps\": 3,\n",
    "            \"use_lazy_mode\": True,\n",
    "            \"overwrite_output_dir\": True,\n",
    "            \"seed\": 42,\n",
    "            \"bf16\": True,\n",
    "            \"report_to\":'tensorboard',\n",
    "            \"deepspeed\": {\n",
    "                \"steps_per_print\": 64,\n",
    "                \"train_batch_size\": \"auto\",\n",
    "                \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "                \"gradient_accumulation_steps\": \"auto\",\n",
    "                \"gradient_checkpoint\": True,\n",
    "                \"memory_efficient_linear\": False,\n",
    "                \"bf16\": {\n",
    "                    \"enabled\": True\n",
    "                },\n",
    "                \"gradient_clipping\": 1.0,\n",
    "                \"zero_optimization\": {\n",
    "                    \"stage\": 3,\n",
    "                    \"overlap_comm\": False,\n",
    "                    \"reduce_scatter\": False,\n",
    "                    \"contiguous_gradients\": False,\n",
    "                    \"stage3_gather_16bit_weights_on_model_save\": True\n",
    "                }\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "\n",
    "    scaling_config = ScalingConfig(num_workers=num_workers,\n",
    "                                   use_gpu=False,\n",
    "                                   resources_per_worker={\"CPU\": 1, \"HPU\": 1})\n",
    "\n",
    "    # Set backend to hccl in TorchConfig\n",
    "    torch_config = TorchConfig(backend=\"hccl\")\n",
    "    runtime_env = {\n",
    "        \"env_vars\": {\n",
    "        }\n",
    "    }\n",
    "\n",
    "    ray.init(runtime_env=runtime_env)\n",
    "\n",
    "    # Initialize a Ray TorchTrainer\n",
    "    trainer = TorchTrainer(\n",
    "        train_loop_per_worker=pretrain_llama,\n",
    "        train_loop_config=pretrain_config,\n",
    "        torch_config=torch_config,\n",
    "        scaling_config=scaling_config\n",
    "    )\n",
    "\n",
    "    result = trainer.fit()\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set some environment variables\n",
    "os.environ[\"RAY_EXPERIMENTAL_NOSET_HABANA_VISIBLE_MODULES\"] = \"0\"\n",
    "main(num_workers=4)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
