{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama model pre-training on HPU\n",
    "\n",
    "In this Jupyter notebook, we will pre-train a [huggyllama/llama-7b](https://huggingface.co/huggyllama/llama-7b) model by using HPU.\n",
    "\n",
    "We will use PyTorch for model training and Ray for distributed training. We will use pre-processed dataset [OSCAR](https://github.com/bigscience-workshop/bigscience/tree/master/data/oscar) from bigscience-workshop. Datasets preparation steps can be found [here](https://github.com/HabanaAI/Megatron-DeepSpeed/tree/main?tab=readme-ov-file#dataset-preparation).\n",
    "\n",
    "[Habana Gaudi AI Processors (HPUs)](https://habana.ai) are AI hardware accelerators designed by Habana Labs. For more information, see [Gaudi Architecture](https://docs.habana.ai/en/latest/Gaudi_Overview/index.html) and [Gaudi Developer Docs](https://developer.habana.ai/).\n",
    "\n",
    "Basic features for this pre-training example are:\n",
    "- [OSCAR](https://github.com/bigscience-workshop/bigscience/tree/master/data/oscar) data for training. Detail [dataset preparation](https://github.com/HabanaAI/Megatron-DeepSpeed/tree/main?tab=readme-ov-file#dataset-preparation-examples).\n",
    "- Running on HPUs, support three execution mode: [\"lazy\", \"eager\", \"eager.compile\"](https://docs.habana.ai/en/latest/PyTorch/Reference/PyTorch_Gaudi_Theory_of_Operations.html).\n",
    "- Pre-training llama model use configuration [huggyllama/llama-7b](https://huggingface.co/huggyllama/llama-7b)\n",
    "- [Megatron-DeepSpeed](https://github.com/microsoft/Megatron-DeepSpeed/tree/main) based data processing.\n",
    "- [`GaudiTrainer`](https://github.com/huggingface/optimum-habana/blob/main/optimum/habana/transformers/trainer.py) based training.\n",
    "- Ray based resource scheduling and management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare environment\n",
    "This example run on single node with 4 HPUs.\n",
    "\n",
    "We recommend using a prebuilt container to run these examples. To run a container, you need Docker. See [Install Docker Engine](https://docs.docker.com/engine/install/) for installation instructions.\n",
    "\n",
    "Next, follow [Run Using Containers](https://docs.habana.ai/en/latest/Installation_Guide/Bare_Metal_Fresh_OS.html?highlight=installer#run-using-containers) to install the Habana drivers and container runtime.\n",
    "\n",
    "### Get docker image\n",
    "``` bash\n",
    "docker pull vault.habana.ai/gaudi-docker/1.15.1/ubuntu22.04/habanalabs/pytorch-installer-2.2.0:latest\n",
    "```\n",
    "### Run docker image\n",
    "``` bash\n",
    "docker run -it --runtime=habana -e HABANA_VISIBLE_DEVICES=all -e OMPI_MCA_btl_vader_single_copy_mechanism=none --cap-add=sys_nice --net=host --ipc=host vault.habana.ai/gaudi-docker/1.15.1/ubuntu22.04/habanalabs/pytorch-installer-2.2.0:latest\n",
    "# maybe should mapping your workspace volumns\n",
    "```\n",
    "### Install dependency\n",
    "``` bash\n",
    "# \"optimum-habana>1.11.1\" if exection mode \"eager\" or \"eager.compile\" \n",
    "# \"ray>=2.20.0\"\n",
    "pip install ray[train] notebook transformers datasets evaluate peft accelerate scikit-learn optimum-habana\n",
    "\n",
    "# install deepspeed\n",
    "pip install git+https://github.com/HabanaAI/DeepSpeed.git@1.15.0\n",
    "\n",
    "# install megatron_core\n",
    "pip install git+https://github.com/microsoft/Megatron-DeepSpeed.git#egg=megatron-core\n",
    "\n",
    "# this notebook verfied with packages' version:\n",
    "# transformers==4.38.2\n",
    "# datasets==2.19.1\n",
    "# evaluate==0.4.2\n",
    "# peft==0.4.0\n",
    "# accelerate==0.27.2\n",
    "# scikit-learn==1.4.2\n",
    "# optimum-habana==1.11.1\n",
    "\n",
    "# deepspeed==0.12.4+hpu.synapse.v1.15.0\n",
    "\n",
    "# megatron_core==0.2.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import os\n",
    "from typing import Any, Dict\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import transformers\n",
    "from transformers import HfArgumentParser, default_data_collator\n",
    "\n",
    "from megatron import get_args, print_rank_0\n",
    "from megatron.core import mpu\n",
    "from megatron.data import gpt_dataset\n",
    "from megatron.initialize import initialize_megatron\n",
    "from megatron.data.data_samplers import build_pretraining_data_loader\n",
    "from megatron.training import build_train_valid_test_datasets, update_train_iters\n",
    "\n",
    "from optimum.habana import GaudiConfig, GaudiTrainer, GaudiTrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build datasets\n",
    "\n",
    "Build train, valid, and test datasets.\n",
    "\n",
    "The training data requires preprocessing. First, place your training data in a loose json format, with one json containing a text sample per line, as `jsonl` format.\n",
    "\n",
    "This notebook mainly focus on how to pre-train model Llama use HPUs, so we do not intend to introduce in detail how to preprocess the data.\n",
    "For more steps of how to preprocess data for megatron-deepspeed pre-training, please visit [here](https://github.com/microsoft/Megatron-DeepSpeed/tree/main?tab=readme-ov-file#data-preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MegatronDataset:\n",
    "    def __call__(self, config):\n",
    "        def _train_valid_test_datasets_provider(train_val_test_num_samples):\n",
    "            \"\"\"Build train, valid, and test datasets.\"\"\"\n",
    "            args = get_args()\n",
    "            print_rank_0(\"> building train, validation, and test datasets \" \"for GPT ...\")\n",
    "            train_ds, valid_ds, test_ds = gpt_dataset.build_train_valid_test_datasets(\n",
    "                data_prefix=args.data_path,\n",
    "                data_impl=args.data_impl,\n",
    "                splits_string=args.split,\n",
    "                train_valid_test_num_samples=train_val_test_num_samples,\n",
    "                seq_length=args.seq_length,\n",
    "                seed=args.seed,\n",
    "                skip_warmup=(not args.mmap_warmup),\n",
    "                train_data_prefix=args.train_data_path,\n",
    "                valid_data_prefix=args.valid_data_path,\n",
    "                test_data_prefix=args.test_data_path,\n",
    "                data_cache_path=args.data_cache_path,\n",
    "            )\n",
    "            print_rank_0(\"> finished creating GPT datasets ...\")\n",
    "\n",
    "            return train_ds, valid_ds, test_ds\n",
    "\n",
    "        args = get_args()\n",
    "        update_train_iters(args)\n",
    "        datasets = build_train_valid_test_datasets(_train_valid_test_datasets_provider)\n",
    "        print_rank_0(datasets)\n",
    "        return datasets\n",
    "\n",
    "\n",
    "def load_datasets(config):\n",
    "    dataset = MegatronDataset()\n",
    "    return dataset(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process dataset to dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MegatronProcesser:\n",
    "    def prepare(self, tokenizer, dataset, **kwargs):\n",
    "        args = get_args()\n",
    "\n",
    "        (train_dataloader, valid_dataloader, test_dataloader) = (None, None, None)\n",
    "\n",
    "        print_rank_0(\"> building train, validation, and test datasets ...\")\n",
    "        iteration = kwargs.get(\"step\", 0)\n",
    "        if iteration:\n",
    "            # passed value is starting step\n",
    "            iteration -= 1\n",
    "            args.consumed_train_samples = iteration * args.global_batch_size\n",
    "            args.consumed_valid_samples = (\n",
    "                (args.iteration // args.eval_interval) * args.eval_iters * args.global_batch_size\n",
    "            )\n",
    "\n",
    "        # Data loader only on rank 0 of each model parallel group.\n",
    "        if args.use_dataset_only or mpu.get_tensor_model_parallel_rank() == 0:\n",
    "            # Build datasets.\n",
    "            train_ds, valid_ds, test_ds = dataset\n",
    "\n",
    "            # Build dataloders.\n",
    "            train_dataloader = build_pretraining_data_loader(train_ds, args.consumed_train_samples)\n",
    "            valid_dataloader = build_pretraining_data_loader(valid_ds, args.consumed_valid_samples)\n",
    "            test_dataloader = build_pretraining_data_loader(test_ds, 0)\n",
    "\n",
    "        return train_dataloader, valid_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load tokenizer\n",
    "\n",
    "Download vocabulary from huggingface.co and cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer(config):\n",
    "    name = config[\"name\"]\n",
    "    load_config = config[\"config\"]\n",
    "    return transformers.AutoTokenizer.from_pretrained(name, **load_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Llama model\n",
    "\n",
    "Download configuration from huggingface.co and cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuggingFaceModelFromConfig:\n",
    "    def __call__(self, config):\n",
    "        name = config[\"name\"]\n",
    "        self.model_config = config.get(\"config\", {})\n",
    "        self.auto_config = None\n",
    "        if name is not None:\n",
    "            self.auto_config = transformers.AutoConfig.from_pretrained(\n",
    "                pretrained_model_name_or_path=name, **self.model_config\n",
    "            )\n",
    "        else:\n",
    "            self.auto_config = transformers.AutoConfig.for_model(**self.model_config)\n",
    "        self.model = transformers.AutoModelForCausalLM.from_config(self.auto_config)\n",
    "\n",
    "        return self.model\n",
    "\n",
    "\n",
    "def load_model(config):\n",
    "    model = HuggingFaceModelFromConfig()\n",
    "    return model(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare trainer\n",
    "\n",
    "- inherit Trainer base on `GaudiTrainer`, with custom train dataloader preparation function.\n",
    "- instance Trainer with `model`, `gaudi_config`, `training_args`, `tokenizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HFCustomerSamplerTrainer(GaudiTrainer):  # type: ignore\n",
    "    def set_sampler(self, sampler):\n",
    "        self.customer_sampler = sampler\n",
    "\n",
    "    def get_train_dataloader(self) -> DataLoader:\n",
    "        if self.train_dataset is None:\n",
    "            raise ValueError(\"Trainer: training requires a train_dataset.\")\n",
    "\n",
    "        train_dataloader, _, _ = self.customer_sampler.prepare(\n",
    "            None, (self.train_dataset, None, None)\n",
    "        )\n",
    "        return train_dataloader\n",
    "\n",
    "\n",
    "def get_trainer(config, training_args, datasets, tokenizer, model):\n",
    "    gaudi_config = GaudiConfig.from_pretrained(\n",
    "        training_args.gaudi_config_name,\n",
    "        cache_dir=config.get(\"cache_dir\", None),\n",
    "        revision=config.get(\"model_revision\", None),\n",
    "        use_auth_token=True if config.get(\"use_auth_token\") else None,\n",
    "    )\n",
    "\n",
    "    train_dataset, eval_dataset, test_dataset = datasets\n",
    "\n",
    "    trainer = HFCustomerSamplerTrainer(\n",
    "        model=model,\n",
    "        gaudi_config=gaudi_config,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=None,\n",
    "        tokenizer=tokenizer,\n",
    "        # Data collator will default to DataCollatorWithPadding, so we change it.\n",
    "        data_collator=default_data_collator,\n",
    "        compute_metrics=None,\n",
    "        preprocess_logits_for_metrics=None\n",
    "    )\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function\n",
    "\n",
    "This function will be executed by each worker during training, with following steps:\n",
    "- initialize megatron.\n",
    "- load datasets with prepared local binary and index files.\n",
    "- prepare dataset processor.\n",
    "- load tokenizer configurations from huggingface.co\n",
    "- instance object of `GaudiTrainingArguments`\n",
    "- load model configurations from huggingface.co\n",
    "- instance object of `GaudiTrainer` with training_args, datasets, tokenizer, and model.\n",
    "- call `train` of trainer.\n",
    "- save model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_llama(config: Dict[str, Any]):\n",
    "\n",
    "    initialize_megatron(ignore_unknown_args=True, external_args=config[\"megatron_config\"], allow_no_cuda=True)\n",
    "\n",
    "    datasets = load_datasets(config[\"datasets\"])\n",
    "\n",
    "    dataprocessor = MegatronProcesser()\n",
    "\n",
    "    tokenizer = load_tokenizer(config[\"tokenizer\"])\n",
    "\n",
    "    training_args = GaudiTrainingArguments(**config[\"training_args\"])\n",
    "\n",
    "    model = load_model(config[\"model\"])\n",
    "\n",
    "    trainer = get_trainer(config, training_args, datasets, tokenizer, model)\n",
    "    trainer.set_sampler(dataprocessor)\n",
    "\n",
    "    result = trainer.train()\n",
    "    trainer.save_model()\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Training Function\n",
    "\n",
    "The `main` function sets up the distributed training environment using Ray and starts the training process. To enable training using HPU, we only need to make the following changes:\n",
    "- Set the exectuion mode for training, supported execution mode are:\n",
    "\n",
    "    - \"lazy\": Deferred execution of graphs, comprising of ops delivered from script op by op similar to Eager mode. It gives the Eager mode experience with performance on Gaudi. Unlike Eager Mode with torch.compile, graph is analyzed in each iteration leading to a higher CPU usage.\n",
    "    - \"eager\": Op-by-op execution as defined in standard PyTorch Eager mode scripts.\n",
    "    - \"eager.compile\": Eager mode extended with `torch.compile` - Similar to Eager mode but extended with wrapping complete or part of model (such as a function) into a graph. Parts that are not wrapped are executed eagerly.\n",
    "\n",
    "    More detail theory can be found [here](https://docs.habana.ai/en/latest/PyTorch/Reference/PyTorch_Gaudi_Theory_of_Operations.html), and detail performance results can be found [here](https://developer.habana.ai/get-started/habana-models-performance/)\n",
    "- Require an HPU for each worker in ScalingConfig\n",
    "- Set backend to `hccl` in TorchConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(num_workers, execution_mode):\n",
    "    import ray\n",
    "    from ray.train import ScalingConfig\n",
    "    from ray.train.torch import TorchTrainer, TorchConfig\n",
    "\n",
    "    # configs for pre-training\n",
    "    pretrain_config = {\n",
    "        \"megatron_config\": {\n",
    "            \"data_path\": [\"/root/workspace/bigscience/data/oscar/zh/tokenized_text_document\"],\n",
    "            \"data_impl\": \"mmap\",\n",
    "            \"micro_batch_size\": 1,\n",
    "            \"global_batch_size\": 4,\n",
    "            \"seq_length\": 2048,\n",
    "            \"use_dataset_only\": True,\n",
    "            # \"vocab_file\": \"/home/user/workspace/data/gpt2-vocab.json\",\n",
    "            \"tokenizer_type\": \"HFTokenizer\",\n",
    "            \"tokenizer_model\": \"huggyllama/llama-7b\",\n",
    "            # \"merge_file\": \"/home/user/workspace/data/gpt2-merges.txt\",\n",
    "            \"eval_interval\": 1000,\n",
    "            \"train_samples\": 300_000_000,\n",
    "            \"split\": \"949,50,1\",\n",
    "        },\n",
    "        \"datasets\": {\n",
    "        },\n",
    "        \"tokenizer\": {\n",
    "            \"name\": \"huggyllama/llama-7b\",\n",
    "            \"config\": {}\n",
    "        },\n",
    "        \"model\": {\n",
    "            \"name\": \"huggyllama/llama-7b\",\n",
    "            \"config\": {\n",
    "                \"torch_dtype\": \"bfloat16\",\n",
    "            },\n",
    "        },\n",
    "        \"training_args\": {\n",
    "            \"per_device_train_batch_size\": 1,\n",
    "            \"per_device_eval_batch_size\": 1,\n",
    "            \"do_train\": True,\n",
    "            \"do_eval\": False,\n",
    "            \"save_strategy\": \"epoch\",\n",
    "            \"save_steps\": 1000,\n",
    "            \"output_dir\": \"/tmp/pretrain-llama\",\n",
    "            \"gaudi_config_name\": \"Habana/gpt2\",\n",
    "            \"use_habana\": True,\n",
    "            \"max_steps\": 100000,\n",
    "            \"throughput_warmup_steps\": 3,\n",
    "            \"use_lazy_mode\": True,\n",
    "            \"overwrite_output_dir\": True,\n",
    "            \"seed\": 42,\n",
    "            \"bf16\": True,\n",
    "            \"report_to\":'tensorboard',\n",
    "            \"deepspeed\": {\n",
    "                \"steps_per_print\": 64,\n",
    "                \"train_batch_size\": \"auto\",\n",
    "                \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "                \"gradient_accumulation_steps\": \"auto\",\n",
    "                \"gradient_checkpoint\": True,\n",
    "                \"memory_efficient_linear\": False,\n",
    "                \"bf16\": {\n",
    "                    \"enabled\": True\n",
    "                },\n",
    "                \"gradient_clipping\": 1.0,\n",
    "                \"zero_optimization\": {\n",
    "                    \"stage\": 3,\n",
    "                    \"overlap_comm\": False,\n",
    "                    \"reduce_scatter\": False,\n",
    "                    \"contiguous_gradients\": False,\n",
    "                    \"stage3_gather_16bit_weights_on_model_save\": True\n",
    "                }\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # if execution mode is eager with compile, must spcified with a compile backend\n",
    "    if execution_mode == \"eager.compile\":\n",
    "        pretrain_config[\"training_args\"].update({\"torch_compile_backend\": \"hpu_backend\"})\n",
    "\n",
    "    scaling_config = ScalingConfig(num_workers=num_workers,\n",
    "                                   use_gpu=False,\n",
    "                                   resources_per_worker={\"CPU\": 1, \"HPU\": 1})\n",
    "\n",
    "    # Set backend to hccl in TorchConfig\n",
    "    torch_config = TorchConfig(backend=\"hccl\")\n",
    "    runtime_env = {\n",
    "        \"env_vars\": {\n",
    "        }\n",
    "    }\n",
    "\n",
    "    ray.init(runtime_env=runtime_env)\n",
    "\n",
    "    # Initialize a Ray TorchTrainer\n",
    "    trainer = TorchTrainer(\n",
    "        train_loop_per_worker=pretrain_llama,\n",
    "        train_loop_config=pretrain_config,\n",
    "        torch_config=torch_config,\n",
    "        scaling_config=scaling_config\n",
    "    )\n",
    "\n",
    "    result = trainer.fit()\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training\n",
    "\n",
    "Finally, we call the `main` function to start the pre-training process.\n",
    "\n",
    "Before calling `main` function, you must set some environment variables.\n",
    "\n",
    "1. The visiable devices. Environment variable `HABANA_VISIBLE_DEVICES` and `HABANA_VISIBLE_MODULES` are used to control the HPU device visiable to application, you must set this two environment variable properly. For more detail usage of `HABANA_VISIBLE_DEVICES`, `HABANA_VISIBLE_MODULES`, please visit [here](https://docs.habana.ai/en/latest/PyTorch/Reference/PT_Multiple_Tenants_on_HPU/Multiple_Dockers_each_with_Single_Workload.html)\n",
    "\n",
    "2. The execution mode. Different execution mode has different runtime performance. The default execution mode is lazy mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set some environment variables\n",
    "os.environ[\"RAY_EXPERIMENTAL_NOSET_HABANA_VISIBLE_MODULES\"] = \"0\"\n",
    "# if using RAY_EXPERIMENTAL_NOSET_HABANA_VISIBLE_MODULES env var\n",
    "# you must set HABANA_VISIBLE_MODULES, such as\n",
    "# os.environ[\"HABANA_VISIBLE_MODULES\"] = \"0,1,2,3\"\n",
    "\n",
    "# execution_mode are [\"lazy\", \"eager\", \"eager.compile\"]\n",
    "execution_mode = \"lazy\"\n",
    "os.environ[\"PT_HPU_LAZY_MODE\"] = \"1\" if execution_mode == \"lazy\" else \"0\"\n",
    "\n",
    "main(num_workers=4, execution_mode=execution_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible outputs\n",
    "\n",
    "``` bash\n",
    "\n",
    "...\n",
    "(RayTrainWorker pid=359077) ============================= HABANA PT BRIDGE CONFIGURATION =========================== \n",
    "(RayTrainWorker pid=359077)  PT_HPU_LAZY_MODE = 1\n",
    "(RayTrainWorker pid=359077)  PT_RECIPE_CACHE_PATH = \n",
    "(RayTrainWorker pid=359077)  PT_CACHE_FOLDER_DELETE = 0\n",
    "(RayTrainWorker pid=359077)  PT_HPU_RECIPE_CACHE_CONFIG = \n",
    "(RayTrainWorker pid=359077)  PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
    "(RayTrainWorker pid=359077)  PT_HPU_LAZY_ACC_PAR_MODE = 1\n",
    "(RayTrainWorker pid=359077)  PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
    "(RayTrainWorker pid=359077) ---------------------------: System Configuration :---------------------------\n",
    "(RayTrainWorker pid=359077) Num CPU Cores : 152\n",
    "(RayTrainWorker pid=359077) CPU RAM       : 1056440348 KB\n",
    "(RayTrainWorker pid=359077) ------------------------------------------------------------------------------\n",
    "\n",
    "...\n",
    "\n",
    "(RayTrainWorker pid=339380) {'loss': nan, 'grad_norm': nan, 'learning_rate': 4.975e-05, 'epoch': 0.0, 'memory_allocated (GB)': 40.39, 'max_memory_allocated (GB)': 93.68, 'total_memory_available (GB)': 94.62}\\\n",
    "(RayTrainWorker pid=339380) {'loss': nan, 'grad_norm': nan, 'learning_rate': 4.9500000000000004e-05, 'epoch': 0.0, 'memory_allocated (GB)': 40.4, 'max_memory_allocated (GB)': 93.68, 'total_memory_available (GB)': 94.62}\n",
    "(RayTrainWorker pid=339380) {'loss': nan, 'grad_norm': nan, 'learning_rate': 4.9250000000000004e-05, 'epoch': 0.0, 'memory_allocated (GB)': 40.42, 'max_memory_allocated (GB)': 93.68, 'total_memory_available (GB)': 94.62}\n",
    "(RayTrainWorker pid=339380) {'loss': nan, 'grad_norm': nan, 'learning_rate': 4.9e-05, 'epoch': 0.0, 'memory_allocated (GB)': 40.42, 'max_memory_allocated (GB)': 93.68, 'total_memory_available (GB)': 94.62}\n",
    "(RayTrainWorker pid=339380) {'loss': nan, 'grad_norm': nan, 'learning_rate': 4.875e-05, 'epoch': 0.0, 'memory_allocated (GB)': 40.4, 'max_memory_allocated (GB)': 93.68, 'total_memory_available (GB)': 94.62}\n",
    "(RayTrainWorker pid=339380) {'loss': nan, 'grad_norm': nan, 'learning_rate': 4.85e-05, 'epoch': 0.0, 'memory_allocated (GB)': 40.4, 'max_memory_allocated (GB)': 93.68, 'total_memory_available (GB)': 94.62}\n",
    "(RayTrainWorker pid=339380) {'loss': nan, 'grad_norm': nan, 'learning_rate': 4.825e-05, 'epoch': 0.0, 'memory_allocated (GB)': 40.45, 'max_memory_allocated (GB)': 93.68, 'total_memory_available (GB)': 94.62}\n",
    "(RayTrainWorker pid=339380) {'loss': nan, 'grad_norm': nan, 'learning_rate': 4.8e-05, 'epoch': 0.0, 'memory_allocated (GB)': 40.41, 'max_memory_allocated (GB)': 93.68, 'total_memory_available (GB)': 94.62}\n",
    "(RayTrainWorker pid=339380) {'loss': nan, 'grad_norm': nan, 'learning_rate': 4.775e-05, 'epoch': 0.0, 'memory_allocated (GB)': 40.43, 'max_memory_allocated (GB)': 93.68, 'total_memory_available (GB)': 94.62}\n",
    "(RayTrainWorker pid=339380) {'loss': nan, 'grad_norm': nan, 'learning_rate': 4.75e-05, 'epoch': 0.0, 'memory_allocated (GB)': 40.4, 'max_memory_allocated (GB)': 93.68, 'total_memory_available (GB)': 94.62}\n",
    "(RayTrainWorker pid=339380) {'loss': nan, 'grad_norm': nan, 'learning_rate': 4.7249999999999997e-05, 'epoch': 0.0, 'memory_allocated (GB)': 40.39, 'max_memory_allocated (GB)': 93.68, 'total_memory_available (GB)': 94.62}\n",
    "(RayTrainWorker pid=339380) {'loss': nan, 'grad_norm': nan, 'learning_rate': 4.7249999999999997e-05, 'epoch': 0.0, 'memory_allocated (GB)': 40.39, 'max_memory_allocated (GB)': 93.68, 'total_memory_available (GB)': 94.62}\n",
    "\n",
    "...\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
